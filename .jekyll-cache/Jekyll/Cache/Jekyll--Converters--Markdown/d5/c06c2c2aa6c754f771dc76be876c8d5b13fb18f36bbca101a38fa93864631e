I"š<h1 id="gan">GAN</h1>
<h2 id="definition">Definition</h2>
<ul>
  <li>GANs : Generative Adversarial Networks
    <ul>
      <li>Generative : ìƒì„±ì˜</li>
      <li>Adversarial : ëŒ€ë¦½ ê´€ê³„ì˜, ì ëŒ€ì ì¸</li>
    </ul>
  </li>
  <li>Generator(ìƒì„±ì) ì™€ Discriminator(ê°ë³„ì) ë‘ ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ
    <ul>
      <li><img src="https://3.bp.blogspot.com/-BgYz6OQc4WU/WchaisOCgOI/AAAAAAAACI0/ONloRtdmVisug_HbkotMbP9tr2hkyfg-ACK4BGAYYCw/s1600/kakao_report2.png" alt="" /></li>
      <li>ì¼ë°˜ì ì¸ ë¹„ìœ  : ì§€í ìœ„ì¡°ë²”ê³¼ ê²½ì°°
        <ul>
          <li>ìœ„ì¡°ë²”ì€ ë”ìš± ì§„ì§œê°™ì€ ê°€ì§œ ìœ„ì¡°ì§€íë¥¼ ë§Œë“œë ¤ê³  í•œë‹¤ : Generator</li>
          <li>ê²½ì°°ì€ ì§€í ê°ë³„ëŠ¥ë ¥ì„ ë†’ì—¬ ìœ„ì¡°ì§€íë¥¼ ì¡ì•„ì•¼ í•œë‹¤ : Discriminator</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ìµœì¢… : ìœ„ì¡°ë²”ì˜ ëŠ¥ë ¥ì´ ì •ì ì— ë‹¬í•˜ë©´, ê²½ì°°ì€ ì§„ì§œì™€ ìœ„ì¡° ì§€íë¥¼ ì°ì–´ì„œ ë§ì¶”ëŠ” ìˆ˜ë°–ì— ì—†ë‹¤.($D(x)=\frac{1}{2},p=0.5$)
    <ul>
      <li>ê²½ì°°ì˜ ëŠ¥ë ¥ì´ ì¢‹ì•„ì§€ë©´, ìœ„ì¡°ë²”ì€ ë” ì •ë°€í•œ ìœ„íë¥¼ ë§Œë“¤ì–´ì•¼í•œë‹¤. -&gt; ìœ„ì¡°ë²”ì˜ ëŠ¥ë ¥ì´ ì¢‹ì•„ì§€ë©´ ê²½ì°°ë„ ê°ë³„ëŠ¥ë ¥ì„ ë†’ì—¬ì•¼í•œë‹¤. -&gt; í”¼ë“œë°±</li>
      <li>ì‹¤ì œë¡œëŠ” ì´ë ‡ê²Œ ì´ìƒì ìœ¼ë¡œ ëŒì•„ê°€ì§€ ì•ŠëŠ”ë‹¤..
        <ul>
          <li>mode-collapse : ìœ„ì¡°ë²”ì´ 1000ì›ë§Œ ê¸°ê°€ë§‰íˆê²Œ ë§Œë“¤ì—ˆê³  ê²½ì°°ì´ êµ¬ë¶„ì„ ëª»í•œë‹¤. -&gt; ê³„ì† 1000ì›ë§Œ ë§Œë“¤ê²ƒì´ë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="equations">Equations</h2>
<ul>
  <li>$x$ : sample from real data</li>
  <li>$z \sim p_z$ : latent variable ($p_z$ : generally gaussian)</li>
  <li>$G(z) \sim p_g$</li>
  <li>objective
    <ul>
      <li>Generator : $p_g=p_{data}$</li>
      <li>Discriminator : $D(x)=1, D(G(z))=0$
\(V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}[\log{D(x;\theta_D)}]+
       \mathbb{E}_{z\sim p_{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]\)</li>
      <li>Dê°€ ì™„ë²½í•  ë•Œ
        <ul>
          <li>$x \sim p_{data}(x), D(x)=0 \implies 1st~term=0$</li>
          <li>$z \sim p_{z}(z), D(G(z))=0 \implies 2nd~term=0$</li>
          <li>$\therefore V(D,G)=0 \implies \max_{D} V(D,G)$</li>
          <li>Dì˜ ëª©ì ì€ V ê°’ì˜ ìµœëŒ€í™”</li>
        </ul>
      </li>
      <li>Gê°€ ì™„ë²½í•  ë•Œ
        <ul>
          <li>1st term ì€ zì™€ ê´€ê³„ì—†ìœ¼ë¯€ë¡œ constant</li>
          <li>$z \sim p_{z}(z), D(G(z))=1 \implies 2nd~term=-\infty$</li>
          <li>$\therefore V(D,G)=-\infty \implies \min_{G} V(D,G)$</li>
          <li>Gì˜ ëª©ì ì€ Vê°’ì˜ ìµœì†Œí™”</li>
        </ul>
      </li>
      <li>re-cap objective : find $G,D$ which satisfy $\min_{G} \max_{D} V(D,G)$</li>
    </ul>
  </li>
  <li>ì„ì˜ì˜ Gì— ëŒ€í•œ ìµœì ì˜ D : $D^{*}<em>G(x) = \frac{p</em>{data}(x)}{p_{data}(x) + p_g(x)}$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\max_D V(D,G) = C(G) = -\log(4) + 2 \cdot JSD(p_{data}</td>
          <td>Â </td>
          <td>p_g)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>re-cap objective : $\min_G C(G) : \min_G JSD(p_{data}</td>
          <td>Â </td>
          <td>p_g)$</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>JSD(Jensen Shannon Divergence) : sum of commuted KL-divergence</li>
      <li>JSD ì¸¡ë„ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œì™€ ë™ì¼</li>
      <li>GëŠ” ì„ì˜ í˜•íƒœì˜ pdf -&gt; non-parametric í•œ Gë¥¼ ë§Œë“¤ê¸° ìœ„í•´ NNì„ ì‚¬ìš© -&gt; GANs</li>
    </ul>
  </li>
</ul>

<h2 id="proscons">Pros/Cons</h2>
<ul>
  <li>GAN : sampler
    <ul>
      <li>ìµœì í™”ì‹œì— $p_g = p_{data}$ ê°€ ë˜ë„ë¡í•˜ëŠ” $G(z) \sim p_g$ì„ ë§Œë“ ë‹¤.
        <ul>
          <li>$z$ì˜ mapping function ì„ êµ¬í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë¯€ë¡œ, $x \sim p_{data}$ ë¥¼ ì§ì ‘ êµ¬í•˜ëŠ” ê²ƒê³¼ëŠ” ë‹¤ë¥´ë‹¤. : ë°ì´í„° ë¶„í¬ë¥¼ ì§ì ‘ êµ¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.</li>
          <li>ë”°ë¼ì„œ, ë°ì´í„° ë¶„í¬ ìì²´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ tracktable likelihood ë¥¼ ê°€ì •í•˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ, likelihood-free í•˜ë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>GAN í•™ìŠµì˜ ì–´ë ¤ì›€
    <ul>
      <li>Convergence
        <ul>
          <li>D,G ë¥¼ ë™ì‹œì— êµ¬í•œë‹¤ -&gt; saddle point ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì´ ì˜ì›ì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.</li>
        </ul>
      </li>
      <li>mode collapse
        <ul>
          <li>NNìœ¼ë¡œ í‘¸ëŠ” í˜„ì‹¤ì ì¸ GANì˜ ê²½ìš°, ë§¤ ë‹¨ê³„ë§ˆë‹¤ ìµœì ì˜ $D^{*}$ ë¥¼ êµ¬í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ, value function ì„ Gì™€ Dì— ëŒ€í•´ ë²ˆê°ˆì•„ê°€ë©´ì„œ í’€ì–´ì•¼í•œë‹¤.
            <ul>
              <li>$G^{*} = \min_{G} \max_{D} V(G,D)$</li>
              <li>G,D ì— ëŒ€í•´ ë²ˆê°ˆì•„ê°€ë©° í’€ê²½ìš°, ìœ„ ì‹ì€ $G^{*} = \max_{D} \min_{G} V(G,D)$ ì™€ ë‹¤ë¥´ì§€ ì•Šë‹¤.
                <ul>
                  <li>$\min_{G}$ ë¥¼ ë¨¼ì € í‘¼ë‹¤. : Dê°€ ê°€ì¥ í—·ê°ˆë ¤ í• ë§Œí•œ ìƒ˜í”Œ í•˜ë‚˜ë§Œ ë§Œë“¤ë©´ ë•¡ -&gt; ì‰¬ìš´ê²ƒë§Œ ë§Œë“œëŠ” Gê°€ ëœë‹¤. -&gt; latent variable $z$ì— ëŒ€í•œ ë³€í™”ê°€ í¬ì§€ ì•Šì€ $G(z)$ê°€ ë§Œë“¤ì–´ì§„ë‹¤.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="training">Training</h2>
<ul>
  <li>$\max_{D} V(G,D)$ ë¥¼ ë¨¼ì € í‘¼ë‹¤.
    <ul>
      <li>ê³ ì •ëœ Gë¥¼ ë‘ê³  ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ Dì—ê²Œ ì œê³µí•œë‹¤.
        <ul>
          <li>ìƒì„±ê¸° ë°ì´í„°ì™€ ë¼ë²¨ : (G(z), 0.0)</li>
          <li>ì§„ì§œ ë°ì´í„°ì™€ ë¼ë²¨ : (x,1.0)</li>
          <li>Discriminator ì—ë§Œ back-prop</li>
          <li>Loss function : $L_D (\theta_G, \theta_D) = -V(G,D) = - \mathbb{E}<em>{x\sim p</em>{data}(x)}[\log{D(x;\theta_D)}] - \mathbb{E}<em>{z\sim p</em>{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]$ : binary crossentropy ì™€ ë™ì¼í•¨</li>
          <li>$\theta_D$ ë§Œ ì—…ë°ì´íŠ¸</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ê³ ì •ëœ Dë¥¼ ë‘ê³  Gë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤
    <ul>
      <li>Loss function : $L_G (\theta_G, \theta_D) = - \mathbb{E}<em>{z\sim p</em>{z}(z)}[\log{(1-D(G(z;\theta_G);\theta_D)}]$</li>
      <li>ìœ„ ì‹ìœ¼ë¡œ í’€ë©´($G=arg\min_{G}L_G$) Gê°€ ì²˜ìŒì— ë§Œë“œëŠ” G(z)ëŠ” ë‹¹ì—°íˆ ì´ìƒí•  í™•ë¥ ì´ ë†’ìœ¼ë¯€ë¡œ, í•™ìŠµ ì´ˆê¸°ì— ê°’ì´ ì˜ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤.</li>
      <li>ë”°ë¼ì„œ, í•¨ìˆ˜ë¥¼ $L_G (\theta_G, \theta_D) = - \mathbb{E}<em>{z\sim p</em>{z}(z)}[\log{D(G(z;\theta_G);\theta_D)}]$ ë¡œ ë°”ê¾¸ì–´, $G=arg\max_{G}L_G$ ì°¾ëŠ” ë¬¸ì œë¡œ ë³€ê²½í•œë‹¤.</li>
      <li>ì´í›„ $\theta_G$ ë§Œ ì—…ë°ì´íŠ¸</li>
    </ul>
  </li>
</ul>

<h2 id="example">Example</h2>
<ul>
  <li>DCGAN(Deep-convolutional GAN)
    <ul>
      <li>Maxpooling, Upsampling ëŒ€ì‹  strides&gt;1 convolution ì„ ì‚¬ìš©í•˜ì—¬ feature map í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ í•¨</li>
      <li>Dense ëŠ” z ë°›ì„ë•Œë§Œ</li>
      <li>Batch normalization ì„ í•˜ì§€ë§Œ, G ì¶œë ¥ê³¼ D ì…ë ¥ì—ëŠ” ì‚¬ìš©</li>
      <li>Generatorì—ì„œëŠ” ì¶œë ¥ì— tanh(MNISTëŠ” sigmoid), ë‚˜ë¨¸ì§€ëŠ” ReLU</li>
      <li>Discriminatorì—ì„œëŠ” ì „ë¬´ Leaky ReLU, alpha=0.2</li>
    </ul>
  </li>
</ul>
:ET