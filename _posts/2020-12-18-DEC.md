---
layout: article
title: DEC
mathjax: true
toc : true
tags : UnsupervisedDeepEmbeddingClustering
---


<h1>Junyuan Xie et al. Unsupervised Deep Embedding for Clustering Analysis (2015)$^{(1)}$ - Implementation (DEC)</h1>

<h3>Introduction</h3>

<h5>Research about Methods of Clustering on Unsupervised Problem - motivation</h5>
For instance, think about this "common" proposition to solve a clustering problem:

Use Euclidean distance on raw pixels to cluster (K-means, for example).

This has proved to be ineffective, which illustrates that the choice of <b>feature space</b> is crucial.

Some work has been done (before 2014) on supervised learning, but not for unsupervised. The aim is to find a method of clustering for unsupervised.
<br />
<br />
<h5>Question</h5>
Can we use a <b>data driven</b> approach to solve for the feature space and cluster memberships <b>jointly?</b>
<br />
<br />
<h5>Proposition</h5>
Choice of feature space : <br />
We will use "Deep Embedding" provided by the mapping learned, and parameterized by a deep neural network.

Unsupervised problem: <br />
To handle this problem, We will define and refine clusters iteratively with an auxiliary target distribution, derived from a soft cluster assignment.
<br />
<br />
<h5>Reforming the Proposition - what is DEC?</h5>
Deep Embedded Clustering is an algorithm that clusters a set of data points in a jointly optimized feature space.

The framework provide a way to learn a representation specialized for clustering without groundtruth cluster membership labels.

The method can be viewed as an unsupervised extension of semisupervised self-training.
<br />
<br />

<h3>Software workflow and network structure</h3>

<br />
<img src="/assets/images/dec_files/dec_networkstruct.png" >
<br />
<img src="/assets/images/dec_files/dec_softwarewf.png" width="600">
<br />

<h3>Details on the pipeline</h3>

<h4>(Pre)training Autoencoder</h4>

<img src="/assets/images/dec_files/dec_autoencoder_decoder.png" >

Simple autoencoder architecture, trained in a traditional way:

loss (least-squares loss): ${\mid \mid x-y \mid \mid}_{2}^{2}$

<h4>Training encoder</h4>

<img src="/assets/images/dec_files/dec_encoder_only.png" >

<h5> 1. Initialize "Cluster Centers"</h5>

Given we have input data ${[x_{i} \in X]}_{i=1}^{n}$, $n$ number of data, in data space $X$.

We pre-trained the autoencoder $f_{\theta} : X \to Z$, learnable parameters $\theta$, latent feature space $Z$.

We define a set of cluster centers ${(\mu_{j} \in Z)}_{j=1}^{k}$, 

$k$ number of cluster centers, each labelled $\mu_{j}$. Then we initialize ${(\mu_{j})}_{j=1}^{k}$, 

by running k-means with 20 restarts, select best solution based on $f_{\theta}(x_{i})$.

<br />
<br />

<h5> 2.1. Evaluate loss: soft assignment $q_{ij}$</h5>

Use the Student's t-distribution as a kernel to measure similiarity between $z_{i}$ and $\mu_{j}$:

$q_{ij} = \frac{(1 + {\mid\mid z_{i} - \mu_{j} \mid\mid}^{2}/\alpha){}^{-(\alpha + 1)/2}}{\sum_{j'}{(1 + {\mid\mid z_{i} - \mu_{j'} \mid\mid}^{2}/{\alpha})}^{-(\alpha + 1)/2}}$

where $z_{i} = f_{\theta}(x_{i}) \in Z$ corresponding to $x_{i} \in X$, $\alpha$ degrees of freedom in Student's t-distribution (fixed at 1 in this example).

$q_{ij}$ can be interpreted as the probability of assigning sample $i$ to cluster $j$ - soft assignment.
<br />
<br />

<h5> 2.2. Evaluate loss: Auxiliary target distribution $p_{i}$</h5>

Raise $q_{i}$ to the second power and normalize by frequency per cluster:

$p_{ij} = \frac{q^{2}_{ij} / f_{j}}{\sum_{j'}q^{2}_{ij'} / f_{j'}}$

where $f_{j'} = \sum_{i}q_{ij}$ are soft cluster frequencies.

Target distribution $P$ is important for the performance of model: the goal is to have $P$ that can:

1. Improve cluster purity
2. Put more emphasis on data points assigned with high confidence
3. Normalize loss contribution of each centroid to prevent large clusters from distorting the hidden feature space.
<br />
<br />

<h5> 2.3. Evaluate loss: Kullback-Leibler Divergence </h5>

Finally, we calculate the KL Divergence loss between the soft assignments $q_{i}$ and the auxiliary distribution $p_{i}$.

The aim to minimize, using results from 2.1 and 2.2:

$L = KL(P{\mid}{\mid}Q) = \sum_{i} \sum_{j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$

"Train by matching the soft assignment to the target distribution"

<h3> Expected results (from paper) </h3>

<br />
<img src="/assets/images/dec_files/dec_fig3.png" >
<br />
With trained cluster centers $\mu_{j}$, model $f_{\theta}$,

Find top 10 scoring candidates $x_{i}$ for each cluster $j$ - according to $dist(f_{\theta}(x_{i}), \mu_{j})$.

<br />
<img src="/assets/images/dec_files/dec_fig5.png">
<br />
Evaluate $f_{\theta}(x_{i})$ at a certain epoch, reduce dimension via tSNE and plot.

We observe that the clusters are moving further away from each other per epoch, showing the effectiveness of KL Divergence as a loss function.

<br />
<img src="/assets/images/dec_files/dec_table2.png">
<br />
Comparison of state-of-art architectures (note that this paper was published in 2015). 
<br />

We use unsupervised clustering accuracy ($ACC$) as evaluation metric:

$ACC = \max_{m} \frac{ \sum^{n}_{i=1} 1 (l_{i} = m(c_{i})) }{n}$

where $l_{i}$ is the ground-truth label, $c_{i}$ cluster assignment produced by the algorithm, $m$ ranges over all possible one-to-one mappings between clusters and labels, on $n$ number of data.


<br />

<h3>Implementations</h3>

<h4>Define classes required for DEC</h4>


```python
import os
import torch
import torch.nn as nn
from torch.autograd import Variable

import torchvision.datasets as dset
import torchvision.transforms as transforms

import torch.nn.functional as F
import torch.optim as optim

from sklearn.cluster import MiniBatchKMeans, KMeans

import numpy as np
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
import random
from sklearn.utils.linear_assignment_ import linear_assignment

import time
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from scipy.spatial import distance
```


```python
use_cuda = torch.cuda.is_available() # true if cuda is installed correctly
nmi = normalized_mutual_info_score # look at side-notes at the end for more description on NMI
  # this will only be used when evaluating (will not impact training / updating weights)
```


```python
def acc(y_true, y_pred):
    """
    Calculate clustering accuracy. Require scikit-learn installed
    # Arguments
        y: true labels, numpy.array with shape `(n_samples,)`
        y_pred: predicted labels, numpy.array with shape `(n_samples,)`
    # Return
        accuracy, in [0,1]
    """
    # solves ACC in 4.2.
    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    
    # form a matrix of predicted label and true label
    D = max(y_pred.max(), y_true.max()) + 1 # D = the bigger number of: number of clusters, number of true labels "classes"
    w = np.zeros((D, D), dtype=np.int64) # 10x10 zeros
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1
    
    from sklearn.utils.linear_assignment_ import linear_assignment
    ind = linear_assignment(w.max() - w) # find 'm'
    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size # final calc
```


```python
class DEC_AE(nn.Module):
    # Autoencoder class - contains model structure / weights
    # Also contains cluster centers, method to calculate q_ij 
    
    def __init__(self, num_classes, num_features):
        # note: num_classes only used to define the cluster center (num of classes in classification data)
        # , num_features determine our "latent" dimension number (n-dimensional space to put the data point onto)
        super(DEC_AE,self).__init__()
        
        self.dropout = nn.Dropout(p=0.1)
        self.fc1 = nn.Linear(28*28,500)
        self.fc2 = nn.Linear(500,500)
        self.fc3 = nn.Linear(500,2000)
        self.fc4 = nn.Linear(2000,num_features)
        self.relu = nn.ReLU()
        self.fc_d1 = nn.Linear(500,28*28)
        self.fc_d2 = nn.Linear(500,500)
        self.fc_d3 = nn.Linear(2000,500)
        self.fc_d4 = nn.Linear(num_features,2000)
        self.alpha = 1.0 # these are hard coded as they are not recommended to fix, stated on paper
        self.clusterCenter = nn.Parameter(torch.zeros(num_classes,num_features)) # 10 x 10 for mnist
            # in a 10-dim plot (space), we have one cluster point (which will have 10 coordinate (axis) values as result)
        self.pretrainMode = True
        
        for m in self.modules(): # initialize weights
            # zero-mean gaussian distribution with s.d. 0.01 - on paper, here we just do xavier
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                torch.nn.init.xavier_uniform(m.weight)
                #torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)

    def setPretrain(self,mode):
        """To set training mode to pretrain or not, 
        so that it can control to run only the Encoder or Encoder+Decoder"""
        # FYI: pretrain uses enc+dec
        self.pretrainMode = mode
    
    def updateClusterCenter(self, cc):
        """
        To update the cluster center. This is a method for pre-train phase.
        When a center is being provided by kmeans, we need to update it so
        that it is available for further training
        :param cc: the cluster centers to update, size of num_classes x num_features
        """
        # take in cc num_classes x num_features (10x10 for mnist), update clusterCenter variable within this class
        self.clusterCenter.data = torch.from_numpy(cc)
    
    def getTDistribution(self,x, clusterCenter):
        """
        student t-distribution, as same as used in t-SNE algorithm.
         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.
         
         :param x: input data, in this context it is encoder output
         :param clusterCenter: the cluster center from kmeans
         """
        # calculate 3.1.1. (1), soft assignment
        print(x.size())
        print(clusterCenter.size())
        xe = torch.unsqueeze(x,1).cuda() - clusterCenter.cuda()
        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))
        q = q ** (self.alpha + 1.0) / 2.0
        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q
        return q
        
    def forward(self,x):
        x = x.view(-1, 1*28*28)
        x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.relu(x)
        x = self.fc4(x)
        x_ae = x
        #if not in pretrain mode, we only need encoder
        if self.pretrainMode == False:
            return x, self.getTDistribution(x,self.clusterCenter)
        
        ##### encoder is done, followed by decoder #####
        x = self.fc_d4(x)
        x = self.relu(x)
        x = self.fc_d3(x)
        x = self.relu(x)
        x = self.fc_d2(x)
        x = self.relu(x)
        x = self.fc_d1(x)
        x_de = x.view(-1,1,28,28)
        
        return x_ae, x_de
```


```python
class DEC:
    """The class for controlling the training process of DEC"""
    
    def __init__(self,n_clusters,alpha=1.0):
        self.n_clusters=n_clusters
        self.alpha = alpha
        
    # calculate p_ij given q_ij (sample i, cluster j)
    @staticmethod
    def target_distribution(q):
        weight = q ** 2 / q.sum(0)
        return Variable((weight.t() / weight.sum(1)).t().data, requires_grad=True)
    
    # calc KLDivergence and return scalar
    @staticmethod
    def kld(q,p):
        res = torch.sum(p*torch.log(p/q),dim=-1)
        return res
    
    # use test loader, calc acc and nmi of (Kmeans(f(x)), y)
    def validateOnCompleteTestData(self,test_loader,model):
        model.eval()
        to_eval = np.array([model(d[0].cuda())[0].data.cpu().numpy() for i,d in enumerate(test_loader)])
        true_labels = np.array([d[1].cpu().numpy() for i,d in enumerate(test_loader)])
        to_eval = np.reshape(to_eval,(to_eval.shape[0]*to_eval.shape[1],to_eval.shape[2]))
        true_labels = np.reshape(true_labels,true_labels.shape[0]*true_labels.shape[1])
        
        # use kmeans to cluster the output to true_labels number of classes
        km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20, n_jobs=4)
        y_pred = km.fit_predict(to_eval)
        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'
                      % (acc(true_labels, y_pred), nmi(true_labels, y_pred)))
        currentAcc = acc(true_labels, y_pred)
        return currentAcc, nmi(true_labels, y_pred)
    
    
    def pretrain(self,train_loader, test_loader, epochs, loss_filename='pretrain_result.txt', save_model_filename='bestModel'):
        
        dec_ae = DEC_AE(10,10).cuda() #auto encoder
        mseloss = nn.MSELoss()
        optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)
        best_acc = 0.0
        best_epoch = -1
        
        f = open(loss_filename, 'w')
        f.close()
        
        for epoch in range(epochs):
            
            print('Pre-train: epoch {}'.format(epoch))
            
            dec_ae.train()
            running_loss=0.0
            loss_list = []
            
            for i,data in enumerate(train_loader):
                
                x, label = data
                x, label = Variable(x).cuda(),Variable(label).cuda()
                optimizer.zero_grad()
                x_ae, x_de = dec_ae(x) # x_ae not used in pretraining
                loss = F.mse_loss(x_de,x,reduce=True) # calc loss, return scalar
                loss.backward()
                optimizer.step()
                
                #x_eval = x.data.cpu().numpy()
                #label_eval = label.data.cpu().numpy()
                loss_list.append(loss.data.cpu().numpy())
                
                # feedback purposes, we can comment this out
                running_loss += loss.data.cpu().numpy()
                if i % 100 == 99:    # print every 100 mini-batches
                    print('[%d, %5d] loss: %.7f' %
                          (epoch + 1, i + 1, running_loss / 100))
                    running_loss = 0.0
            
            #now we evaluate the accuracy with AE (pretraining module), per epoch
            dec_ae.eval()
            currentAcc, currentNMI = self.validateOnCompleteTestData(test_loader, dec_ae)
            if currentAcc > best_acc:                
                torch.save(dec_ae,'{}'.format(save_model_filename))
                best_acc = currentAcc
                best_epoch = epoch
            
            f = open(loss_filename, 'a')
            f.write('epoch:{} avg_loss:{} acc:{} nmi:{}\n'.format(epoch, np.mean(loss_list), currentAcc, currentNMI))
            f.close()
        
        print('Pretrain done. Best epoch: {}'.format(best_epoch))
    
    # mbk - minibatchkmeans, x - data in batch, model (get encoded part only)
    def clustering(self,mbk,x,model):
        model.eval()
        y_pred_ae,_ = model(x)
        y_pred_ae = y_pred_ae.data.cpu().numpy()
        y_pred = mbk.partial_fit(y_pred_ae) #seems we can only get a centre from batch, kmeans based on encoded part
        
        self.cluster_centers = mbk.cluster_centers_ # save, keep the cluster centers
        model.updateClusterCenter(self.cluster_centers)
    
    def train(self,train_loader, test_loader, epochs, loss_filename='second_train_result.txt', 
              load_pretrained_filename='bestModel', save_ckpt_filename='final_model_checkpoint',
             save_final_filename='final_model_lastepoch'):
        """This method will start training for DEC cluster"""
        ct = time.time()
        model = torch.load("{}".format(load_pretrained_filename)).cuda()
        model.setPretrain(False) # we do not need decoded part, instead we need student t-dist similarity measure (q_ij)
        optimizer = optim.SGD([{'params': model.parameters()}, ],lr = 0.01, momentum=0.9) # use parameters from before
        
        print('Initializing cluster center with pre-trained weights')
        mbk = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=batch_size)
        got_cluster_center = False
        
        best_acc = 0.0
        
        f = open(loss_filename, 'w')
        f.close()
        
        for epoch in range(epochs):
            
            print('Train: epoch {}'.format(epoch))
            
            epoch_starttime = time.time()
            loss_list = []
            
            for i,data in enumerate(train_loader):
                x, label = data
                x = Variable(x).cuda()
                optimizer.zero_grad()
                               
                #step 1 - get cluster center from batch
                #here we are using minibatch kmeans to be able to cope with larger dataset.
                if epoch > 1:
                    got_cluster_center = True
                    
                if not got_cluster_center: # epoch 0, epoch 1
                    loss_list = [-1]
                    self.clustering(mbk,x,model)
                    
                else: # start training from epoch 2
                    model.train()
                    #now we start training with acquired cluster center
                    feature_pred,q = model(x)
                    #get target distribution
                    p = self.target_distribution(q)
                    loss = self.kld(q,p).mean()
                    loss.backward()
                    optimizer.step()
                    loss_list.append(loss.data.cpu().numpy())
            
            currentAcc, currentNMI = self.validateOnCompleteTestData(test_loader,model)
            
            if currentAcc > best_acc:
                torch.save(model,'{}'.format(save_ckpt_filename))
                best_acc = currentAcc
                print('Saving model to {}:\n epoch:{}, accuracy:{}'.format(save_ckpt_filename, epoch, best_acc))
            
            avg_loss = np.mean(loss_list)
            f = open(loss_filename, 'a')
            f.write('epoch:{} avg_loss:{} acc:{} nmi:{}\n'.format(epoch, avg_loss, currentAcc, currentNMI))
            f.close()
            
            epoch_time_taken = time.time() - epoch_starttime
            print('Epoch {} complete.\n'
                  'Time taken: {:.3f}, '
                  'Avg Loss: {:.4f}'.format(epoch, epoch_time_taken, avg_loss))
                    
        print('Training done. Saving final model.. {}'.format(save_final_filename))
        torch.save(model,'{}'.format(save_final_filename))
        
        time_taken = time.time() - ct
        print('Total Time taken: {:.4f} s'.format(time_taken))
```

<h4>Load Training/Testing Data</h4>


```python
## load mnist dataset
root = './data'

if not os.path.exists(root):
    os.mkdir(root)

trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) # data value: -0.5~0.5 from 0~255

# if not exist, download mnist dataset
train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)
test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)

batch_size = 256
train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)
```

<h4>Start Training</h4>


```python
pretrain_loss_filename = 'pretrain.txt'
maintrain_loss_filename = 'train.txt'
pretrain_model_filename = 'pretrain_bestmodel'
ckpt_model_filename = 'ckpt_model'
final_model_filename = 'final_model'
```


```python
random.seed(7)

# use classes defined above
dec = DEC(10)
```


```python
dec.pretrain(train_loader, test_loader, 200, loss_filename=pretrain_loss_filename, save_model_filename=pretrain_model_filename)
```


```python
dec.train(train_loader, test_loader, 200, loss_filename=maintrain_loss_filename, load_pretrained_filename=pretrain_model_filename, 
          save_ckpt_filename=ckpt_model_filename, save_final_filename=final_model_filename)
```

<h4>Evaluation Module</h4>

<h5>Learning Curve</h5>


```python
# log file format: mode= 1.loss 2.acc 3.nmi

def read_file_and_plot(filename, output=None, mode=1, title='Learning Curve', y_label='Loss'):
    with open(filename, 'r') as f:
        lines = f.readlines()
    
    losses = []

    for l in lines:
        value = l.split(' ')[mode].split(':')[1]
        if float(value) >= 0:
            losses.append(float(value))
    
    plt.title(title)
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel(y_label)
    if output is not None:
        plt.savefig(output)
    plt.show()
    return min(losses)
```


```python
read_file_and_plot('first_train_result/1_train.txt', output=None, mode=1, y_label='Loss')
```

<br />
<img src="/assets/images/dec_files/dec_train_loss.png" >
<br />

<h5>Load model, extract cluster centers</h5>


```python
model = torch.load("first_train_result/1221_ckpt_model")
```


```python
clustercenter = model.clusterCenter.detach().numpy()
```

<h5>Extract features from testing data using the loaded model</h5>


```python
def extract_features(net, dataloader):
    
    net = net.cuda()
    net.eval()
    net.setPretrain(False)
    
    result_nparr = None
    
    with torch.no_grad():
        
        for i, (x, y) in enumerate(dataloader):
            x = Variable(x).cuda()
            features, _ = net(x)
            
            if result_nparr is None:
                result_nparr = features.detach().to('cpu').numpy()
            else:
                result_nparr = np.concatenate((result_nparr, features.detach().to('cpu').numpy()), axis=0)
            
    return result_nparr
```


```python
test_features = extract_features(model,test_loader) # get output from a neural network
```


```python
# Taken part of DEC.validateOnCompleteTestData, add label according to the features by kmeans
# This part is only required if the data is labelled.

km = KMeans(n_clusters=10, n_init=20, n_jobs=4)
y_pred = km.fit_predict(test_features)
```


```python
acc(test_labels, y_pred) # measure accuracy to test if model output fits well w.r.t. true labels
```




    0.8698



<h5>Experiment 1 : Extract Images</h5>


```python
# pick out the data (point) based on indexes of data(base)

def imageplot(numpy_arr, name=None):
    x_size = np.max([10, numpy_arr.shape[1] / 50])
    y_size = np.max([7, numpy_arr.shape[0] / 50])
    plt.figure(figsize=(x_size,y_size))
    plt.imshow(numpy_arr, cmap='gray')
    plt.axis('off')
    if name is not None:
        plt.savefig(name)
    plt.show()

def display_candidates(data, indexes, plot_filename=None):
    
    total_image_np = None
    
    for i in range(indexes.shape[0]):
        total_image_row = None
        
        for j in range(indexes.shape[1]):
            if total_image_row is None:
                total_image_row = data[indexes[i,j]]
            else:
                total_image_row = np.hstack((total_image_row, data[indexes[i,j]]))
        
        if total_image_np is None:
            total_image_np = total_image_row
        else:
            total_image_np = np.vstack((total_image_np, total_image_row))
    
    print(total_image_np.shape)
    imageplot(total_image_np, name=plot_filename)
```

<h5>1.1. Extract images closest to the cluster centers</h5>


```python
# measure euclidean distance of each center, of each feature point
# input : the centers of clusters (num_centers x num_features), array of feature points (num_points x num_features)
# output : matrix of distances (euclidean) - num_centers x num_points

def measure_euclidean(centers, features):
    
    distance_matrix = []
    
    for i in range(centers.shape[0]):
        temp_dist_list = []
        
        for j in range(features.shape[0]):
            distance_value = distance.euclidean(features[j,:], centers[i,:])
            temp_dist_list.append(distance_value)
        
        distance_matrix.append(temp_dist_list)
    
    return np.array(distance_matrix)
```


```python
distance_matrix = measure_euclidean(clustercenter, test_features) # measure distance between feature and cluster center
sort_index = np.argsort(distance_matrix, axis=1) # make index array of smallest -> largest distance from cluster center
```


```python
display_candidates(test_loader.dataset.test_data.numpy(), sort_index[:,:20])
 # display testing data that is closest to the cluster center (in features dimension, euclidean distance)
```

<img src="/assets/images/dec_files/dec_mnist_model_candidates.png" >

<h5>1.2. For supervised : get images which are labelled incorrectly</h5>


```python
def extract_incorrect_details(y_true, y_pred):
    # [the name is acc_aux due to most of code being from acc] - now renamed to extract_incorrect_details
    # but the function is to return incorrect_list,
    # list of indexes which pred and true labels conflict from
    
    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    
    # form a matrix of predicted label and true label
    D = max(y_pred.max(), y_true.max()) + 1 # D = the bigger number of: number of clusters, number of true labels "classes"
    w = np.zeros((D, D), dtype=np.int64) # 10x10 zeros
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1
    
    ind = linear_assignment(w.max() - w) # find 'm'
    #return w, ind

    kmean_true = [j for i, j in ind] # true labels list w.r.t. pred (k means) label
    
    incorrect_list = []
    
    for j in range(D):
        incorrect_list.append([])
    
    for i in range(y_pred.shape[0]):
        if test_labels[i] != kmean_true[y_pred[i]]:
            incorrect_list[y_pred[i]].append(i)
    
    # kmean_true - index: kmeans label, value: true label
    # incorrect_list - incorrect list w.r.t. kmean (pred) label
    return kmean_true, incorrect_list
```


```python
truelabel_list, incorrect_list = extract_incorrect_details(test_labels, y_pred)
 # truelabel_list : ex. [3,1,2,0] -> kmean 0 = true label 3, kmean 1 = true label 1, ...
 # incorrect_list : list of indexes in data that test_labels and kmeans collide
```


```python
# make list of indexes to plot (2D numpy array)

number_of_incorrects = 17 # extract these amount only (for plotting)

incorrect_indexes_trimmed = []

for i in range(len(incorrect_list)): # 0..9 true label
    incorrect_indexes_trimmed.append(incorrect_list[truelabel_list.index(i)][:number_of_incorrects])

incorrect_indexes_trimmed = np.array(incorrect_indexes_trimmed)
```


```python
display_candidates(test_loader.dataset.test_data.numpy(), incorrect_indexes_trimmed, plot_filename='incorrect_label.png')
```

<img src="/assets/images/dec_files/dec_incorrect_label.png" >

<h5>Experiment 2: use tSNE to plot test features in 2D, visualize cluster</h5>


```python
test_features_tsne = TSNE(n_components=2).fit_transform(test_features)
```


```python
def plot_scatter(features_2d, label_array, filename=None):
    # features_2d (num_data x 2)
    # label_array (num_data x num_classes)
    total = 0
    
    plt.figure(figsize=(20,20))
    plt.xlim((-100,100))
    plt.ylim((-100,100))
    
    # color, according to label_array
    for i in np.unique(label_array):
        indices = np.where(label_array == i)[0]
        plt.scatter(features_2d[indices,0], features_2d[indices,1], marker='+', label='{}'.format(i))
        total += indices.shape[0]
    
    plt.legend()
    if filename is not None:
        plt.savefig(filename)
    plt.show()
    
    return total # assurance that all data points are taken to consideration
```


```python
#plot_scatter(test_features_tsne, test_labels, filename='TSNE_true.png') # colour data according to true label (supervised only)
plot_scatter(test_features_tsne, y_pred) # colour data according to prediction cluster
```

<img src="/assets/images/dec_files/dec_TSNE_kmeans.png" >

<br />

Side notes:

NMI: https://course.ccs.neu.edu/cs6140sp15/7_locality_cluster/Assignment-6/NMI.pdf

Key concepts: t-Student Distribution, Kullback-Leibler Divergence, Stochastic Gradient Descent, k-means clustering, stacked autoencoder

<br />
References:

(1)
<cite>
Junyuan Xie, Ross Girshick, Ali Farhadi. Unsupervised Deep Embedding for Clustering Analysis. ICML, 2015.
</cite>

Disclaimer:

This work is done purely for educational purposes thus non-commercial. 

<br />
