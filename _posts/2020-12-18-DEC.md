---
layout: article
title: DEC
mathjax: true
toc : true
tags : UnsupervisedDeepEmbeddingClustering
---




<h1>Junyuan Xie et al. Unsupervised Deep Embedding for Clustering Analysis (2015)$^{(1)}$ - Implementation (DEC)</h1>

<h3>Introduction</h3>

<h5>Research about Methods of Clustering on Unsupervised Problem - motivation</h5>
For instance, think about this "common" proposition to solve a clustering problem:

Use Euclidean distance on raw pixels to cluster (K-means, for example).

This has proved to be ineffective, which illustrates that the choice of <b>feature space</b> is crucial.

Some work has been done (before 2014) on supervised learning, but not for unsupervised. The aim is to find a method of clustering for unsupervised.
<br />
<br />
<h5>Question</h5>
Can we use a <b>data driven</b> approach to solve for the feature space and cluster memberships <b>jointly?</b>
<br />
<br />
<h5>Proposition</h5>
Choice of feature space : <br />
We will use "Deep Embedding" provided by the mapping learned, and parameterized by a deep neural network.

Unsupervised problem: <br />
To handle this problem, We will define and refine clusters iteratively with an auxiliary target distribution, derived from a soft cluster assignment.
<br />
<br />
<h5>Reforming the Proposition - what is DEC?</h5>
Deep Embedded Clustering is an algorithm that clusters a set of data points in a jointly optimized feature space.

The framework provide a way to learn a representation specialized for clustering without groundtruth cluster membership labels.

The method can be viewed as an unsupervised extension of semisupervised self-training.
<br />

<h3>Software workflow and network structure</h3>

<br />
<img src="/assets/images/dec_files/dec_networkstruct.png" >
<br />
<img src="/assets/images/dec_files/dec_softwarewf.png" width="600">
<br />

<h3>Details on the pipeline</h3>

<h4>(Pre)training Autoencoder</h4>

<img src="/assets/images/dec_files/dec_autoencoder_decoder.png" >

Simple autoencoder architecture, trained in a traditional way:

loss (least-squares loss): ${\mid\mid x-y \mid\mid}_{2}^{2}$

<h4>Training encoder</h4>

<img src="/assets/images/dec_files/dec_encoder_only.png" >

<h5> 1. Initialize "Cluster Centers"</h5>

Given we have input data ${\{x_{i} \in X\}}_{i=1}^{n}$, $n$ number of data, in data space $X$.

We pre-trained the autoencoder $f_{\theta} : X \to Z$, learnable parameters $\theta$, latent feature space $Z$.

We define a set of cluster centers ${\{\mu_{j} \in Z\}}_{j=1}^{k}$, $k$ number of cluster centers, each labelled $\mu_{j}$.

We initialize ${\{\mu_{j}\}}_{j=1}^{k}$ by running k-means with 20 restarts, select best solution based on $f_{\theta}(x_{i})$.
<br />
<br />

<h5> 2.1. Evaluate loss: soft assignment $q_{ij}$</h5>

Use the Student's t-distribution as a kernel to measure similiarity between $z_{i}$ and $\mu_{j}$:

$q_{ij} = \frac{{(1 + \mid\mid z_{i} - \mu_{j} \mid\mid {}^{2}/{\alpha})}^{-(\alpha + 1)/2}}{\sum_{j'}{(1 + \mid\mid z_{i} - \mu_{j'} \mid\mid{}^{2}/{\alpha})}^{-(\alpha + 1)/2}}$ ,

where $z_{i} = f_{\theta}(x_{i}) \in Z$ corresponding to $x_{i} \in X$, $\alpha$ degrees of freedom in Student's t-distribution (fixed at 1 in this example).

$q_{ij}$ can be interpreted as the probability of assigning sample $i$ to cluster $j$ - soft assignment.
<br />
<br />

<h5> 2.2. Evaluate loss: Auxiliary target distribution $p_{i}$</h5>

Raise $q_{i}$ to the second power and normalize by frequency per cluster:

$p_{ij} = \frac{q^{2}_{ij} / f_{j}}{\sum_{j'}q^{2}_{ij'} / f_{j'}}$

where $f_{j'} = \sum_{i}q_{ij}$ are soft cluster frequencies.

Target distribution $P$ is important for the performance of model: the goal is to have $P$ that can:

1. Improve cluster purity
2. Put more emphasis on data points assigned with high confidence
3. Normalize loss contribution of each centroid to prevent large clusters from distorting the hidden feature space.
<br />
<br />

<h5> 2.3. Evaluate loss: Kullback-Leibler Divergence </h5>

Finally, we calculate the KL Divergence loss between the soft assignments $q_{i}$ and the auxiliary distribution $p_{i}$.

The aim to minimize, using results from 2.1 and 2.2:

$L = KL(P{\mid}{\mid}Q) = \sum_{i} \sum_{j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$

"Train by matching the soft assignment to the target distribution"

<h3> Expected results (from paper) </h3>

<br />
<img src="/assets/images/dec_files/dec_fig3.png" >
With trained cluster centers $\mu_{j}$, model $f_{\theta}$,

Find top 10 scoring candidates $x_{i}$ for each cluster $j$ - according to $dist(f_{\theta}(x_{i}), \mu_{j})$.

<br />
<img src="/assets/images/dec_files/dec_fig5.png">
Evaluate $f_{\theta}(x_{i})$ at a certain epoch, reduce dimension via tSNE and plot.

We observe that the clusters are moving further away from each other per epoch, showing the effectiveness of KL Divergence as a loss function.

<br />
<img src="/assets/images/dec_files/dec_table2.png">
Comparison of state-of-art architectures (note that this paper was published in 2015). 
<br />

We use unsupervised clustering accuracy ($ACC$) as evaluation metric:

$ACC = \max_{m} \frac{\sum^{n}_{i=1} 1\{l_{i} = m(c_{i})\}}{n}$

where $l_{i}$ is the ground-truth label, $c_{i}$ cluster assignment produced by the algorithm, $m$ ranges over all possible one-to-one mappings between clusters and labels, on $n$ number of data.


<br />

<h3>Implementations</h3>


```python
import os
import torch
import torch.nn as nn
from torch.autograd import Variable

import torchvision.datasets as dset
import torchvision.transforms as transforms

import torch.nn.functional as F
import torch.optim as optim

from sklearn.cluster import MiniBatchKMeans, KMeans

import time

import numpy as np
from sklearn.metrics import normalized_mutual_info_score
```


```python
use_cuda = torch.cuda.is_available() # true if cuda is installed correctly
nmi = normalized_mutual_info_score # look at side-notes at the end for more description on NMI
  # this will only be used when evaluating (will not impact training / updating weights)
```


```python
class DEC_AE(nn.Module):
    
    def __init__(self, num_classes, num_features):
        # note: num_classes only used to define the cluster center (num of classes in classification data)
        # , num_features determine our "latent" dimension number (n-dimensional space to put the data point onto)
        super(DEC_AE,self).__init__()
        
        self.dropout = nn.Dropout(p=0.1)
        self.fc1 = nn.Linear(28*28,500)
        self.fc2 = nn.Linear(500,500)
        self.fc3 = nn.Linear(500,2000)
        self.fc4 = nn.Linear(2000,num_features)
        self.relu = nn.ReLU()
        self.fc_d1 = nn.Linear(500,28*28)
        self.fc_d2 = nn.Linear(500,500)
        self.fc_d3 = nn.Linear(2000,500)
        self.fc_d4 = nn.Linear(num_features,2000)
        self.alpha = 1.0 # these are hard coded as they are not recommended to fix, stated on paper
        self.clusterCenter = nn.Parameter(torch.zeros(num_classes,num_features)) # 10 x 10 for mnist
            # in a 10-dim plot (space), we have one cluster point (which will have 10 coordinate (axis) values as result)
        self.pretrainMode = True
        
        for m in self.modules(): # initialize weights
            # zero-mean gaussian distribution with s.d. 0.01 - on paper, here we just do xavier
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                torch.nn.init.xavier_uniform(m.weight)
                #torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)

    def setPretrain(self,mode):
        """To set training mode to pretrain or not, 
        so that it can control to run only the Encoder or Encoder+Decoder"""
        # FYI: pretrain uses enc+dec
        self.pretrainMode = mode
    
    def updateClusterCenter(self, cc):
        """
        To update the cluster center. This is a method for pre-train phase.
        When a center is being provided by kmeans, we need to update it so
        that it is available for further training
        :param cc: the cluster centers to update, size of num_classes x num_features
        """
        # take in cc num_classes x num_features (10x10 for mnist), update clusterCenter variable within this class
        self.clusterCenter.data = torch.from_numpy(cc)
    
    def getTDistribution(self,x, clusterCenter):
        """
        student t-distribution, as same as used in t-SNE algorithm.
         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.
         
         :param x: input data, in this context it is encoder output
         :param clusterCenter: the cluster center from kmeans
         """
        # calculate 3.1.1. (1), soft assignment
        print(x.size())
        print(clusterCenter.size())
        xe = torch.unsqueeze(x,1).cuda() - clusterCenter.cuda()
        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))
        q = q ** (self.alpha + 1.0) / 2.0
        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q
        return q
        
    def forward(self,x):
        x = x.view(-1, 1*28*28)
        x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.relu(x)
        x = self.fc4(x)
        x_ae = x
        #if not in pretrain mode, we only need encoder
        if self.pretrainMode == False:
            return x, self.getTDistribution(x,self.clusterCenter)
        
        ##### encoder is done, followed by decoder #####
        x = self.fc_d4(x)
        x = self.relu(x)
        x = self.fc_d3(x)
        x = self.relu(x)
        x = self.fc_d2(x)
        x = self.relu(x)
        x = self.fc_d1(x)
        x_de = x.view(-1,1,28,28)
        
        return x_ae, x_de
```


```python
def acc(y_true, y_pred):
    """
    Calculate clustering accuracy. Require scikit-learn installed
    # Arguments
        y: true labels, numpy.array with shape `(n_samples,)`
        y_pred: predicted labels, numpy.array with shape `(n_samples,)`
    # Return
        accuracy, in [0,1]
    """
    # solves ACC in 4.2.
    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    
    # form a matrix of predicted label and true label
    D = max(y_pred.max(), y_true.max()) + 1 # D = the bigger number of: number of clusters, number of true labels "classes"
    w = np.zeros((D, D), dtype=np.int64) # 10x10 zeros
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1
    
    from sklearn.utils.linear_assignment_ import linear_assignment
    ind = linear_assignment(w.max() - w) # find 'm'
    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size # final calc
```


```python
class DEC:
    """The class for controlling the training process of DEC"""
    
    def __init__(self,n_clusters,alpha=1.0):
        self.n_clusters=n_clusters
        self.alpha = alpha
        
    # calculate p_ij given q_ij (sample i, cluster j)
    @staticmethod
    def target_distribution(q):
        weight = q ** 2 / q.sum(0)
        #print('q',q)
        return Variable((weight.t() / weight.sum(1)).t().data, requires_grad=True)
    
    # print acc
    def logAccuracy(self,pred,label):
        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==| logAccuracy'
          % (acc(label, pred), nmi(label, pred)))
    
    # calc KLDivergence and return scalar
    @staticmethod
    def kld(q,p):
        res = torch.sum(p*torch.log(p/q),dim=-1)
        return res
    
    # use test loader, calc acc and nmi of (Kmeans(f(x)), y)
    def validateOnCompleteTestData(self,test_loader,model):
        model.eval()
        to_eval = np.array([model(d[0].cuda())[0].data.cpu().numpy() for i,d in enumerate(test_loader)])
        true_labels = np.array([d[1].cpu().numpy() for i,d in enumerate(test_loader)])
        to_eval = np.reshape(to_eval,(to_eval.shape[0]*to_eval.shape[1],to_eval.shape[2]))
        true_labels = np.reshape(true_labels,true_labels.shape[0]*true_labels.shape[1])
        
        # use kmeans to cluster the output to true_labels number of classes
        km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20, n_jobs=4)
        y_pred = km.fit_predict(to_eval)
        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==| validateOnCompleteTestData'
                      % (acc(true_labels, y_pred), nmi(true_labels, y_pred)))
        currentAcc = acc(true_labels, y_pred)
        return currentAcc, nmi(true_labels, y_pred)
    
    
    def pretrain(self,train_loader, test_loader, epochs, loss_filename='pretrain_result.txt'):
        
        dec_ae = DEC_AE(10,10).cuda() #auto encoder
        mseloss = nn.MSELoss()
        optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)
        best_acc = 0.0
        
        f = open(loss_filename, 'w')
        f.close()
        
        for epoch in range(epochs):
            
            print('Pre-train: epoch {}'.format(epoch))
            
            dec_ae.train()
            running_loss=0.0
            loss_list = []
            
            for i, data in enumerate(train_loader):
                
                x, label = data
                x, label = Variable(x).cuda(),Variable(label).cuda()
                optimizer.zero_grad()
                x_ae, x_de = dec_ae(x) # x_ae not used in pretraining
                loss = F.mse_loss(x_de,x,reduce=True) # calc loss, return scalar
                loss.backward()
                optimizer.step()
                
                #x_eval = x.data.cpu().numpy()
                #label_eval = label.data.cpu().numpy()
                loss_list.append(loss.data.cpu().numpy())
                
                # feedback purposes, we can comment this out
                running_loss += loss.data.cpu().numpy()
                if i % 100 == 99:    # print every 100 mini-batches
                    print('[%d, %5d] loss: %.7f' %
                          (epoch + 1, i + 1, running_loss / 100))
                    running_loss = 0.0
            
            #now we evaluate the accuracy with AE (pretraining module), per epoch
            dec_ae.eval()
            currentAcc, currentNMI = self.validateOnCompleteTestData(test_loader, dec_ae)
            if currentAcc > best_acc:                
                torch.save(dec_ae,'bestModel'.format(best_acc))
                best_acc = currentAcc
            
            f = open(loss_filename, 'a')
            f.write('epoch:{} avg_loss:{} acc:{} nmi:{}\n'.format(epoch, np.mean(loss_list), currentAcc, currentNMI))
            f.close()
    
    # mbk - minibatchkmeans, x - data in batch, model (get encoded part only)
    def clustering(self,mbk,x,model):
        model.eval()
        y_pred_ae,_ = model(x)
        y_pred_ae = y_pred_ae.data.cpu().numpy()
        y_pred = mbk.partial_fit(y_pred_ae) #seems we can only get a centre from batch, kmeans based on encoded part
        
        self.cluster_centers = mbk.cluster_centers_ # save, keep the cluster centers
        model.updateClusterCenter(self.cluster_centers)
    
    def train(self,train_loader, test_loader, epochs, loss_filename='second_train_result.txt'):
        """This method will start training for DEC cluster"""
        ct = time.time()
        model = torch.load("bestModel").cuda()
        model.setPretrain(False) # we do not need decoded part, instead we need student t-dist similarity measure
        optimizer = optim.SGD([{'params': model.parameters()}, ],lr = 0.01, momentum=0.9) # use parameters from before
        
        print('Initializing cluster center with pre-trained weights')
        mbk = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=batch_size)
        got_cluster_center = False
        
        f = open(loss_filename, 'w')
        f.close()
        
        for epoch in range(epochs):
            
            print('Train: epoch {}'.format(epoch))
            
            loss_list = []
            
            for i,data in enumerate(train_loader):
                x, label = data
                x = Variable(x).cuda()
                optimizer.zero_grad()
                               
                #step 1 - get cluster center from batch
                #here we are using minibatch kmeans to be able to cope with larger dataset.
                if epoch > 1:
                    got_cluster_center = True
                    
                if not got_cluster_center: # epoch 0, epoch 1
                    loss_list = [-1]
                    self.clustering(mbk,x,model)
                    
                else: # start training from epoch 2
                    model.train()
                    #now we start training with acquired cluster center
                    feature_pred,q = model(x)
                    #get target distribution
                    p = self.target_distribution(q)
                    #print('q',q,'p',p)
                    loss = self.kld(q,p).mean()
                    loss.backward()
                    optimizer.step()
                    loss_list.append(loss.data.cpu().numpy())
            
            currentAcc, currentNMI = self.validateOnCompleteTestData(test_loader,model)
            
            print(loss_list[0])
            f = open(loss_filename, 'a')
            f.write('epoch:{} avg_loss:{} acc:{} nmi:{}\n'.format(epoch, np.mean(loss_list), currentAcc, currentNMI))
            f.close()
```


```python
## load mnist dataset
root = './data'

if not os.path.exists(root):
    os.mkdir(root)

trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) # data value: -0.5~0.5 from 0~255

# if not exist, download mnist dataset
train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)
test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)

batch_size = 256
train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)
```


```python
#now start training
import random
random.seed(7)

dec = DEC(10)
dec.pretrain(train_loader, test_loader, 200)
dec.train(train_loader, test_loader, 200)
```

<br />

Side notes:

NMI: https://course.ccs.neu.edu/cs6140sp15/7_locality_cluster/Assignment-6/NMI.pdf

Key concepts: t-Student Distribution, Kullback-Leibler Divergence, Stochastic Gradient Descent, k-means clustering, stacked autoencoder

<br />
References:

(1)
<cite>
Junyuan Xie, Ross Girshick, Ali Farhadi. Unsupervised Deep Embedding for Clustering Analysis. ICML, 2015.
</cite>

Disclaimer:

This work is done purely for educational purposes thus non-commercial. 

<br />
Further questions:
    
1. 4.3. Concept of "parameter that controls annealing speed" ${\lambda}$

2. Statement of 1, 3.1 and 4.3 seems to contradict themselves when talking about "refining clusters". <br /> Question: Are cluster centroids ${{\{\mu_{j}\}}_{j=1}^{k}}$ updated during training (in epochs)?

<br />
Future work:

1. Examine implementations carefully: cluster centers? lambda? Mis-translation of Caffe (original work) and edited work (Pytorch)?

2. Implement evaluation module and visualize our own training results
