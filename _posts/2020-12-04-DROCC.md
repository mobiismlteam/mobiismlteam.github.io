
---
layout: article
title: DROCC
mathjax: true
toc : true
tags : NoveltyDetection
---


# DROCC : Deep Robust One-Class Classification

## Intuition
![png](/assets/images/drocc_files/drocc_intuition.PNG)

## Algorithm
![png](/assets/images/drocc_files/drocc_algo.PNG)

### Loss function
![png](/assets/images/drocc_files/drocc_lossfunction.PNG)

## Adversarial search source code


```python
x_adv = torch.randn(x_train_data.shape).to(self.device).detach().requires_grad_()
x_adv_sampled = x_adv + x_train_data

for step in range(self.ascent_num_steps):
    with torch.enable_grad():

        new_targets = torch.zeros(batch_size, 1).to(self.device)
        new_targets = torch.squeeze(new_targets)
        new_targets = new_targets.to(torch.float)

        logits = self.model(x_adv_sampled)         
        logits = torch.squeeze(logits, dim = 1)
        new_loss = F.binary_cross_entropy_with_logits(logits, new_targets)

        grad = torch.autograd.grad(new_loss, [x_adv_sampled])[0]
        grad_norm = torch.norm(grad, p=2, dim = tuple(range(1, grad.dim())))
        grad_norm = grad_norm.view(-1, *[1]*(grad.dim()-1))
        grad_normalized = grad/grad_norm 
    with torch.no_grad():
        x_adv_sampled.add_(self.ascent_step_size * grad_normalized)

    if (step + 1) % 10==0:
        # Project the normal points to the set N_i(r)
        h = x_adv_sampled - x_train_data
        norm_h = torch.sqrt(torch.sum(h**2, 
                                        dim=tuple(range(1, h.dim()))))
        alpha = torch.clamp(norm_h, self.radius, 
                            self.gamma * self.radius).to(self.device)
        # Make use of broadcast to project h
        proj = (alpha/norm_h).view(-1, *[1] * (h.dim()-1))
        h = proj * h
        x_adv_sampled = x_train_data + h  #These adv_points are now on the surface of hyper-sphere

adv_pred = self.model(x_adv_sampled)
adv_pred = torch.squeeze(adv_pred, dim=1)
adv_loss = F.binary_cross_entropy_with_logits(adv_pred, (new_targets * 0))
```

## Model


```python
from __future__ import print_function
import os
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from collections import OrderedDict
from process_cifar import CIFAR10_Dataset
from edgeml_pytorch.trainer.drocc_trainer import DROCCTrainer
```


```python
class CIFAR10_LeNet(nn.Module):

    def __init__(self):
        super(CIFAR10_LeNet, self).__init__()

        self.rep_dim = 128
        self.pool = nn.MaxPool2d(2, 2)

        self.conv1 = nn.Conv2d(3, 32, 5, bias=False, padding=2)
        self.bn2d1 = nn.BatchNorm2d(32, eps=1e-04, affine=False)
        self.conv2 = nn.Conv2d(32, 64, 5, bias=False, padding=2)
        self.bn2d2 = nn.BatchNorm2d(64, eps=1e-04, affine=False)
        self.conv3 = nn.Conv2d(64, 128, 5, bias=False, padding=2)
        self.bn2d3 = nn.BatchNorm2d(128, eps=1e-04, affine=False)
        self.fc1 = nn.Linear(128 * 4 * 4, self.rep_dim, bias=False)
        self.fc2 = nn.Linear(self.rep_dim, int(self.rep_dim/2), bias=False)
        self.fc3 = nn.Linear(int(self.rep_dim/2), 1, bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool(F.leaky_relu(self.bn2d1(x)))
        x = self.conv2(x)
        x = self.pool(F.leaky_relu(self.bn2d2(x)))
        x = self.conv3(x)
        x = self.pool(F.leaky_relu(self.bn2d3(x)))
        x = x.view(x.size(0), -1)
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

## Hyper parameters


```python
class Args:
    normal_class = 0 # type=int, default=0, metavar='N', help='CIFAR10 normal class index'
    batch_size = 256 # type=int, default=128, metavar='N', help='batch size for training'
    epochs = 40 # type=int, default=100, metavar='N', help='number of epochs to train'
    only_ce_epochs = 10 # type=int, default=50, metavar='N', help='number of epochs to train with only CE loss'
    ascent_num_steps = 100 # type=int, default=50, metavar='N', help='Number of gradient ascent steps'
    hd = 128 # type=int, default=128, metavar='N', help='Num hidden nodes for LSTM model'
    lr = 0.001 # type=float, default=0.001, metavar='LR', help='learning rate'
    ascent_step_size = 0.001 # type=float, default=0.001, metavar='LR', help='step size of gradient ascent'
    mom = 0.99 # type=float, default=0.99, metavar='M', help='momentum'
    model_dir = 'log' # default='log', help='path where to save checkpoint'
    one_class_adv = 1 # type=int, default=1, metavar='N', help='adv loss to be used or not, 1:use 0:not use(only CE)'
    radius = 8 # type=float, default=0.2, metavar='N', help='radius corresponding to the definition of set N_i(r)'
    lamda = 1 # type=float, default=1, metavar='N', help='Weight to the adversarial loss'
    reg = 0 # type=float, default=0, metavar='N', help='weight reg'
    eval = 0 # type=int, default=0, metavar='N', help='whether to load a saved model and evaluate (0/1)'
    optim = 0 # type=int, default=0, metavar='N', help='0 : Adam 1: SGD'
    gamma = 1 # type=float, default=2.0, metavar='N', help='r to gamma * r projection for the set N_i(r)'
    data_path = '.' # type=str, default='.'
    metric = 'AUC' # type=str, default='AUC'
```


```python
def adjust_learning_rate(epoch, total_epochs, only_ce_epochs, learning_rate, optimizer):
        """Adjust learning rate during training.

        Parameters
        ----------
        epoch: Current training epoch.
        total_epochs: Total number of epochs for training.
        only_ce_epochs: Number of epochs for initial pretraining.
        learning_rate: Initial learning rate for training.
        """
        #We dont want to consider the only ce 
        #based epochs for the lr scheduler
        epoch = epoch - only_ce_epochs
        drocc_epochs = total_epochs - only_ce_epochs
        # lr = learning_rate
        if epoch <= drocc_epochs:
            lr = learning_rate * 0.01
        if epoch <= 0.80 * drocc_epochs:
            lr = learning_rate * 0.1
        if epoch <= 0.40 * drocc_epochs:
            lr = learning_rate
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        return optimizer
```

## Main training


```python
torch.set_printoptions(precision=5)

args_ = Args()

model_dir = args_.model_dir
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

dataset = CIFAR10_Dataset("data", args_.normal_class)
train_loader, test_loader = dataset.loaders(batch_size=args_.batch_size)
model = CIFAR10_LeNet().to(device)
model = nn.DataParallel(model)
x_train_data = 0
x_adv_sampled = 0

if args_.optim == 1:
    optimizer = optim.SGD(model.parameters(),
                              lr=args_.lr,
                              momentum=args_.mom)
    print("using SGD")
else:
    optimizer = optim.Adam(model.parameters(),
                           lr=args_.lr)
    print("using Adam")

trainer = DROCCTrainer(model, optimizer, args_.lamda, args_.radius, args_.gamma, device)

if args_.eval == 0:
    # Training the model 
    x_train_data, x_adv_sampled, loss = trainer.train(train_loader, test_loader, args_.lr, adjust_learning_rate, args_.epochs,
        metric=args_.metric, ascent_step_size=args_.ascent_step_size, only_ce_epochs = 0)

    trainer.save(args_.model_dir)
    
else:
    if os.path.exists(os.path.join(args_.model_dir, 'model.pt')):
        trainer.load(args_.model_dir)
        print("Saved Model Loaded")
    else:
        print('Saved model not found. Cannot run evaluation.')
        exit()
    score, label_score = trainer.test(test_loader, 'AUC')
    print('Test AUC: {}'.format(score))
```

    Files already downloaded and verified
    Files already downloaded and verified
    using Adam
    Epoch: 0, CE Loss: 0.7015791535377502, AdvLoss: 0.6782273650169373, AUC: 0.7596701111111113
    Epoch: 1, CE Loss: 0.41726183891296387, AdvLoss: 0.38988617062568665, AUC: 0.748073777777778
    Epoch: 2, CE Loss: 0.016368327662348747, AdvLoss: 0.01352552231401205, AUC: 0.642144
    Epoch: 3, CE Loss: 0.00041665430762805045, AdvLoss: 0.0002151713997591287, AUC: 0.8102603333333335
    Epoch: 4, CE Loss: 2.9664955945918337e-05, AdvLoss: 5.739830157835968e-05, AUC: 0.7902892222222222
    Epoch: 5, CE Loss: 2.020062311203219e-05, AdvLoss: 7.414194442389999e-06, AUC: 0.7784740555555557
    Epoch: 6, CE Loss: 1.0124302207259461e-05, AdvLoss: 9.130786565947346e-06, AUC: 0.7940844444444444
    Epoch: 7, CE Loss: 6.9329398684203625e-06, AdvLoss: 6.582492915185867e-06, AUC: 0.7890107777777777
    Epoch: 8, CE Loss: 6.579746241186513e-06, AdvLoss: 4.987209649698343e-06, AUC: 0.7906569999999999
    Epoch: 9, CE Loss: 5.223497282713652e-06, AdvLoss: 4.009505573776551e-06, AUC: 0.7870652777777777
    Epoch: 10, CE Loss: 4.3728323362302035e-06, AdvLoss: 3.3941503261303296e-06, AUC: 0.7826726666666667
    Epoch: 11, CE Loss: 3.5068974284513388e-06, AdvLoss: 3.4849442727136193e-06, AUC: 0.7877991111111111
    Epoch: 12, CE Loss: 3.6011695101478836e-06, AdvLoss: 2.7018720629712334e-06, AUC: 0.7921879444444444
    Epoch: 13, CE Loss: 2.920326096500503e-06, AdvLoss: 2.4462794954160927e-06, AUC: 0.7916833333333333
    Epoch: 14, CE Loss: 2.2025085399945965e-06, AdvLoss: 2.3404397779813735e-06, AUC: 0.7920451111111111
    Epoch: 15, CE Loss: 2.763195197985624e-06, AdvLoss: 1.8489038211555453e-06, AUC: 0.7867494444444445
    Epoch: 16, CE Loss: 1.971476422113483e-06, AdvLoss: 1.9182407413609326e-06, AUC: 0.7854850000000001
    Epoch: 17, CE Loss: 1.8915770851890557e-06, AdvLoss: 2.2903529952600366e-06, AUC: 0.7915674444444445
    Epoch: 18, CE Loss: 2.0855104594375007e-06, AdvLoss: 2.0698580556199886e-06, AUC: 0.7794703333333334
    Epoch: 19, CE Loss: 1.7406335928171757e-06, AdvLoss: 1.9611404695751844e-06, AUC: 0.7929842222222222
    Epoch: 20, CE Loss: 2.0735442376462743e-06, AdvLoss: 2.2424524104280863e-06, AUC: 0.7869197777777777
    Epoch: 21, CE Loss: 2.0655941170844017e-06, AdvLoss: 1.8631825469128671e-06, AUC: 0.782581611111111
    Epoch: 22, CE Loss: 2.0824525108764647e-06, AdvLoss: 2.165715613955399e-06, AUC: 0.7896265555555555
    Epoch: 23, CE Loss: 1.6637831095067668e-06, AdvLoss: 1.943893039424438e-06, AUC: 0.7919805555555556
    Epoch: 24, CE Loss: 1.7890639583129087e-06, AdvLoss: 1.9109984350507148e-06, AUC: 0.7864263333333333
    Epoch: 25, CE Loss: 1.7706436210573884e-06, AdvLoss: 2.003354666157975e-06, AUC: 0.7919241666666668
    Epoch: 26, CE Loss: 1.7134440213339985e-06, AdvLoss: 1.7820797211243189e-06, AUC: 0.7965070000000001
    Epoch: 27, CE Loss: 1.7593089296497055e-06, AdvLoss: 1.7107198573285132e-06, AUC: 0.7903612777777776
    Epoch: 28, CE Loss: 2.1613457192870555e-06, AdvLoss: 1.6987814888125286e-06, AUC: 0.7869728333333333
    Epoch: 29, CE Loss: 2.4499627215845976e-06, AdvLoss: 1.8697309087656322e-06, AUC: 0.7951905555555555
    Epoch: 30, CE Loss: 1.8740746554612997e-06, AdvLoss: 1.6757554703872302e-06, AUC: 0.7840148888888889
    Epoch: 31, CE Loss: 1.7950208075490082e-06, AdvLoss: 1.7002132608467946e-06, AUC: 0.7829268888888888
    Epoch: 32, CE Loss: 1.692480395831808e-06, AdvLoss: 1.6644535207888111e-06, AUC: 0.7874344444444444
    Epoch: 33, CE Loss: 1.7054010186257074e-06, AdvLoss: 1.6690337361069396e-06, AUC: 0.7901204444444445
    Epoch: 34, CE Loss: 2.0332511212473037e-06, AdvLoss: 1.9101498764939606e-06, AUC: 0.7906604444444445
    Epoch: 35, CE Loss: 1.9709277694346383e-06, AdvLoss: 1.946329803104163e-06, AUC: 0.7923063888888888
    Epoch: 36, CE Loss: 1.8734601781034144e-06, AdvLoss: 2.1268897398840636e-06, AUC: 0.7864405555555556
    Epoch: 37, CE Loss: 1.956727601282182e-06, AdvLoss: 1.7732094192979275e-06, AUC: 0.7877801111111111
    Epoch: 38, CE Loss: 1.5815066944924183e-06, AdvLoss: 1.738178298182902e-06, AUC: 0.7861248888888888
    Epoch: 39, CE Loss: 1.9975211671408033e-06, AdvLoss: 1.7303854065175983e-06, AUC: 0.7870927222222222
    
    Best test AUC: 0.8102603333333335


### Loss plot


```python
plt.axis((0,40,0,0.0001))
plt.xlabel('epoch')
plt.ylabel('Loss')
plt.plot(loss)
```




    [<matplotlib.lines.Line2D at 0x7fed26a5b2b0>]




![png](/assets/images/drocc_files/output_17_1.png) 



## Visualization normal, generated abnormal samples


```python
import numpy as np
import matplotlib.pyplot as plt
from six.moves import cPickle 

def draw_CIFAR10_Dataset(normal, abnormal):
    
    X_n = normal.transpose(0,2,3,1).astype("uint8")
    X_ab = abnormal.transpose(0,2,3,1).astype("uint8")

    fig, axes1 = plt.subplots(2,10,figsize=(40,8))
    i = np.random.choice(range(len(X_n)), 10)
    
    for k,n in zip(range(10),i):
        axes1[0][k].set_axis_off()
        axes1[0][k].imshow(X_n[n])
    for k,n in zip(range(10),i):
        axes1[1][k].set_axis_off()
        axes1[1][k].imshow(X_ab[n])
        
    fig.text(0.09, 0.70, 'normal',  fontsize=20)
    fig.text(0.09, 0.30, 'abnormal',  fontsize=20)
    
normal_data_set = x_train_data.detach().cpu().numpy()
abnormal_data_set = x_adv_sampled.detach().cpu().numpy()

draw_CIFAR10_Dataset(normal_data_set, abnormal_data_set)
```


    
![png](/assets/images/drocc_files/output_19_0.png)
    


## PCA


```python
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

# data flatten
normal_data_flat = normal_data_set.reshape((normal_data_set.shape[0],-1))
abnormal_data_flat = abnormal_data_set.reshape((abnormal_data_set.shape[0],-1))

# normalization 
scaler = MinMaxScaler()
scaler.fit(normal_data_flat)
normal_data_norm = pd.DataFrame(scaler.transform(normal_data_flat))

scaler.fit(abnormal_data_flat)
abnormal_data_norm = pd.DataFrame(scaler.transform(abnormal_data_flat))

# run PCA 
pca = PCA(n_components=2) 
printcipalComponents = pca.fit_transform(normal_data_flat)
normal_data_norm['PCA1'] = printcipalComponents.transpose()[0]
normal_data_norm['PCA2'] = printcipalComponents.transpose()[1]

pca = PCA(n_components=2) 
printcipalComponents = pca.fit_transform(abnormal_data_flat)
abnormal_data_norm['PCA1'] = printcipalComponents.transpose()[0]
abnormal_data_norm['PCA2'] = printcipalComponents.transpose()[1] 
```


```python
# Scatter Plot
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot()
ax.scatter(normal_data_norm['PCA1'], normal_data_norm['PCA2'], c='blue', s=5, alpha=1, label = 'normal')
ax.scatter(abnormal_data_norm['PCA1'], abnormal_data_norm['PCA2'], c='red', s=5, alpha=0.7, label = 'abnormal')
ax.legend(scatterpoints=5,
           bbox_to_anchor=(1, 0.7), loc=2, borderaxespad=1.,
           ncol=1,
           fontsize=14)
plt.show()
```


    
![png](/assets/images/drocc_files/output_22_0.png)
    



```python

```
