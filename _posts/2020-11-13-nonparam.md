---
layout: article
title: NonParam
mathjax: true
toc : true
tags : UnsupervisedFeatureLearning
---



<h1>Zhirong Wu et al. Unsupervised Feature Learning via Non-Parametric Instance Discrimination (2018)$^{(1)}$ - Implementation</h1>

<h3>Motivation</h3>
<br />
<img src="/assets/images/nonparam_files/np_motivation.png" width="600">
<br />
<h5>Observation of a classification model</h5>
When the image of a leopard is tested on a supervised learning model, the responses are produced as output above.<br />
The highest responses are leopard, and those that look similar to a leopard,<br />
the lowest responses were those that look nothing like a leopard.
<br />
<br />
<h5>Theory Crafting</h5>
Typical discriminative learning methods do not instruct the model to learn the similarity among semantic categories,<br />
but they appear to discover the apparent similarity automatically when learning.
<br />
<br />
<h5>Reforming The Theory Crafting Done Above</h5>
As the semantic annotations are mere classes that are independent of each other by principle,<br />
the apparent similarity must be learned from the visual data themselves.
<br />
<br />
<h5>Question we would like to answer</h5>
Following the last paragraph..<br />
Can we learn to discriminate the individual instances, without any notion of semantic categories?
<br />
<br />
<br />

<h3>The Result of the Application, and Our Objective</h3>

<br />
<img src="/assets/images/nonparam_files/np_interest.png" width="1000">
<br />

We define the input (in other words, testing data) as to the trained model as query.<br />
Given the query, the 10 closest instances from the training set is extracted above.<br />
<br />
For the successful cases, all top 10 results show the same entity (same category as we say for the classification model) as the query. <br />
We observe that even in the failure cases, there are some features that are similar e.g. color, texture, pattern etc. 
<br />
<br />
<br />

<h3>The Pipeline</h3>
<br />
<img src="/assets/images/nonparam_files/np_implementation_pipeline.png" width="1000">
<br />
Above is the training network. <br />

In testing, we calculate the similarity between the query and each element in the memory bank and output top "k" candidates. <br />
<br />
<br />

<h3>Result of Our Training (sample)</h3>
<br />
<img src="/assets/images/nonparam_files/np_res_norecompute.png" >
<br />

<h3>Workflow and Implementations</h3>
<br />
<img src="/assets/images/nonparam_files/np_software_workflow.png">
<br />


```python
# Required common libraries and global variables

import torch
import torch.nn as nn
from torch.autograd import Function
import torch.nn.functional as F
import torch.backends.cudnn as cudnn

import matplotlib.pyplot as plt
import numpy as np
import math
import time
import os

device = 'cuda'
low_dim = 128
```
<br />
<h4>1. Data Preparation</h4>
<br />
Used CIFAR-10 Dataset.

For better performance, several combinations of routine, random transformations were done on the training set (see code).

Training / Testing dataset is loaded into DataLoader which is used for training ('batch size' amount of data).
<br />


```python
from PIL import Image
import torchvision.datasets as datasets

# extension from torchvision.datasets.CIFAR10 library
# we modify the __getitem__ method so that we can also get access to index number of the data when enumerating over the dataset
# the rest we can use the methods given by the default library.
class CIFAR10Instance(datasets.CIFAR10):
    
    def __getitem__(self, index):
        if self.train:
            img, target = self.train_data[index], self.train_labels[index]
        else:
            img, target = self.test_data[index], self.test_labels[index]

        # doing this so that it is consistent with all other datasets
        # to return a PIL Image
        img = Image.fromarray(img)

        if self.transform is not None:
            img = self.transform(img)

        if self.target_transform is not None:
            target = self.target_transform(target)

        return img, target, index
```


```python
import torchvision.transforms as transforms

# the combination of transformations that gets performed randomly to each data in the set

transform_train = transforms.Compose([
    transforms.RandomResizedCrop(size=32, scale=(0.2,1.)), # crop a part of image, then enlarge to original shape
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4), # change color according to:
        # brightness factor, contrast, saturation, hue
    transforms.RandomGrayscale(p=0.2), # change RGB image to grayscale with probability of 0.2
    #transforms.RandomHorizontalFlip(), # not used in paper, so skip
    transforms.ToTensor(), 
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # value becomes of range of around -2.7~2.7
        # normalize RGB value, with (meanR, meanG, meanB), (stdR, stdG, stdB)
        # not exactly sure of why these values are used, but we presume these values were optimized when the author of paper was experimenting
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        # same value with transform_train for consistency purposes.
])
```


```python
# if not in root directory, CIFAR dataset is installed into the specified directory
# train : define if the set we instantiate is the training or the validation set
# transform : define how the data transformation is performed (per data point)

# dataloader: takes in a dataset of type torchvision.datasets
# define batch size of how many data the loader should provide in one enumerate process
# define if the dataset should be shuffled before processing further, and number of cpu "workers"

trainset = CIFAR10Instance(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
ndata = trainset.__len__()

testset = CIFAR10Instance(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)
```
<br />

<h4>2. CNN Backbone model (ResNet)</h4>
<br />
We will use ResNet18 as our backbone model.
<br />
<br />


```python
# Classes below are used to make resnet.
# torch related notes: look at the forward() to get a grasp of the general architecture and how the input is processed through
# init() to see how the layers are initialized (parameters of layer)

class Normalize(nn.Module):
# define l(power) norm, l2 norm is used at the end of the network.
    def __init__(self, power=2):
        super(Normalize, self).__init__()
        self.power = power
    
    def forward(self, x):
        norm = x.pow(self.power).sum(1, keepdim=True).pow(1./self.power)
        out = x.div(norm)
        return out


class BasicBlock(nn.Module):
# convolution block used for resnet18,34
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        # define normal conv layers
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        
        # define shortcut within the conv block
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            # if stride or planes dimension do not match, we cannot perform addition so we cut it in half
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out)) # the "normal" part 
        out += self.shortcut(x) # add with shortcut part
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
# convolution block used for resnet50 upwards
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet(nn.Module):
# total resnet network
    def __init__(self, block, num_blocks, low_dim=128):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64) # beginning part of the resnet
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) # "conv blocks" 1~4
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, low_dim)
        self.l2norm = Normalize(2)

    def _make_layer(self, block, planes, num_blocks, stride):
        # supplementary method to design conv blocks
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        # x is the input of machine learning (of train data dimension)
        print(x.size())
        out = F.relu(self.bn1(self.conv1(x))) # first convolution part before the main part of the network (conv blocks)
        out = self.layer1(out) # block 1 : 32x32x64
        out = self.layer2(out) # block 2 : 16x16x128
        out = self.layer3(out) # block 3 : 8x8x256
        out = self.layer4(out) # block 4 : 4x4x512
        out = F.avg_pool2d(out, 4) # avg pool (4x4): 1x1x512
        out = out.view(out.size(0), -1) # 512, flatten
        out = self.linear(out) # 128, "fc" - reduce dimension to dimension of memory bank
        out = self.l2norm(out) # l2 norm applied
        return out


def ResNet18(low_dim=128):
    return ResNet(BasicBlock, [2,2,2,2], low_dim)

def ResNet34(low_dim=128):
    return ResNet(BasicBlock, [3,4,6,3], low_dim)

def ResNet50(low_dim=128):
    return ResNet(Bottleneck, [3,4,6,3], low_dim)

def ResNet101(low_dim=128):
    return ResNet(Bottleneck, [3,4,23,3], low_dim)

def ResNet152(low_dim=128):
    return ResNet(Bottleneck, [3,8,36,3], low_dim)
```


```python
net = ResNet18(low_dim)

if device == 'cuda':
    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))
    cudnn.benchmark = True
```

<h5>Net architecture</h5>
<img src="/assets/images/nonparam_files/np_resnet18_explained.png">
<br />

<h4>3. Non-parametric softmax classifier, with noise-contrastive estimation - setting memory bank</h4>
<br />

Three classes are defined below:
<br />

1. AliasMethod

Helper class with method to pick out which indexes to use as "noise" in noise contrastive estimation (NCE, in short) at random

2. NCEFunction

Evaluate P(i|v) (*3.4) given features in a batch, parameters (nce_k, tau, ...), indexes (of noise), memory bank from NCEAverage

3. NCEAverage

Store and manage memory bank (self), indexes to use as "noise" in NCE (AliasMethod), use NCEFunction class to carry out calculations to work out P(i|v) (*3.4)

<br />

Using these classes, we define a function to evaluate P(i|v), which will be used to complete our objective function later, and a place to store our memory bank.


```python
class AliasMethod(object):
    '''
        From: https://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/
    '''
    def __init__(self, probs):

        if probs.sum() > 1:
            probs.div_(probs.sum())
        K = len(probs)
        self.prob = torch.zeros(K)
        self.alias = torch.LongTensor([0]*K)

        # Sort the data into the outcomes with probabilities
        # that are larger and smaller than 1/K.
        smaller = []
        larger = []
        for kk, prob in enumerate(probs):
            self.prob[kk] = K*prob
            if self.prob[kk] < 1.0:
                smaller.append(kk)
            else:
                larger.append(kk)

        # Loop though and create little binary mixtures that
        # appropriately allocate the larger outcomes over the
        # overall uniform mixture.
        while len(smaller) > 0 and len(larger) > 0:
            small = smaller.pop()
            large = larger.pop()

            self.alias[small] = large
            self.prob[large] = (self.prob[large] - 1.0) + self.prob[small]

            if self.prob[large] < 1.0:
                smaller.append(large)
            else:
                larger.append(large)

        for last_one in smaller+larger:
            self.prob[last_one] = 1

        
    def cuda(self): 
        self.prob = self.prob.cuda()
        self.alias = self.alias.cuda()

    def draw(self, N):
        '''
            Draw N samples from multinomial
        '''
        K = self.alias.size(0)

        kk = torch.zeros(N, dtype=torch.long, device=self.prob.device).random_(0, K) # array of 1 dimension N consisting values between 0~K (random)
            # the result of indexes at the end depends on the result of this
        prob = self.prob.index_select(0, kk) # select probability values at kk indexes (of self.prob) - result is the same dimension as kk
        alias = self.alias.index_select(0, kk) # select alias - dim "kk", note that self.prob and self.alias do not change.
        
        # b is whether a random number is greater than q
        b = torch.bernoulli(prob) # result is either 1 or 0 for each element (same dim as prob, or kk)
        oq = kk.mul(b.long()) # either value is equal to kk (representing index) or 0
        oj = alias.mul((1-b).long()) # will be 0 if oq at that index exist, or alias of small value (which was assigned large at initialization)

        return oq + oj
```


```python
class NCEFunction(Function):
    @staticmethod
    def forward(self, x, y, memory, idx, params):
        # from NCEAverage, handles calculations from 3.2 in paper
        # given features from data, indexes from data, memory bank (stored as buffer in NCEAverage), 
        # indexes from multinomial, params from NCEAverage instance buffer
        
        K = int(params[0].item()) # nce_k (m)
        T = params[1].item() # tau
        Z = params[2].item() # Z (initial) from eq 5
        #momentum = params[3].item() # exists just for reference
        
        batchSize = x.size(0)
        outputSize = memory.size(0) # number of 'classes' - number of training data (indexes)
        inputSize = memory.size(1) # feature dimension per data in memory bank

        # change value of the first column of each row to be the true label (index)
        # so if we iterate each row of idx and x, first value of a row in idx represents the true label of that row of x
        idx.select(1,0).copy_(y.data) 

        # flatten the idx array to 1D,
        # get the matrix from memory bank for each value in idx (indexes)
        # put it into weight array and compose it into 3D array
        weight = torch.index_select(memory, 0, idx.view(-1)) 
        weight.resize_(batchSize, K+1, inputSize) # inputSize == feature dimension of a data in memory bank

        # inner product - perform (*3.1)
        out = torch.bmm(weight, x.data.resize_(batchSize, inputSize, 1)) # weight: f, x: v
        # T: tau .. batchSize * self.K+1 dimensional - 3.2(4) nominator (*3.2)
        out.div_(T).exp_()
        x.data.resize_(batchSize, inputSize) # restore to original shape (only transformed for matrix multiplication purposes)
        
        # this loop is only entered once (when initializing Z)
        if Z < 0:
            params[2] = out.mean() * outputSize # Z from paper 3.2(5) - denominator for 3.2(4) (*3.3)
            Z = params[2].item()
            print("normalization constant Z is set to {:.1f}".format(Z))

        out.div_(Z).resize_(batchSize, K+1) # 3.2(4) complete (*3.4)
            # output dimension : 2D - batchsize x (K+1)

        self.save_for_backward(x, memory, y, weight, out, params) # required for back-prop

        return out

    @staticmethod
    def backward(self, gradOutput):
        # performed as loss.backward() - back-propagation, required to implement in torch.autograd.Function class
        # gradOutput - derivative of loss w.r.t. the layer output (same dimension as output from forward method)
        
        x, memory, y, weight, out, params = self.saved_tensors
            # from save_for_backward at forward method, x, y, memory same as forward()
            # params : same values, or modified at forward (*3.3)
            # weight : values of memory bank, stored according to indexes specified by idx at forward method
            # out : output of forward method (*3.4)
        
        # self-explanatory variable names, explained on forward method
        K = int(params[0].item())
        T = params[1].item()
        Z = params[2].item()
        momentum = params[3].item()
        batchSize = gradOutput.size(0)
        
        # gradients d Pm / d linear = exp(linear) / Z
        gradOutput.data.mul_(out.data)
        # add temperature
        gradOutput.data.div_(T)

        gradOutput.data.resize_(batchSize, 1, K+1)
        
        # gradient of linear
        gradInput = torch.bmm(gradOutput.data, weight) # batchsize x 1 x K+1 , batchsize x K+1 x latent_dim 
        gradInput.resize_as_(x)
        # gradInput : derivative of loss w.r.t. layer input (output of this method)
        # - dimension should be same as x (2D : batchsize x latent_dim)

        # update the non-parametric data - memory bank
        weight_pos = weight.select(1, 0).resize_as_(x) # get existing weight from memory bank, of the batch data
        weight_pos.mul_(momentum) # reduce, by portion defined by momentum
        weight_pos.add_(torch.mul(x.data, 1-momentum)) # add the rest of the "portion", which is newly computed from the backbone
        w_norm = weight_pos.pow(2).sum(1, keepdim=True).pow(0.5) # find the l2 norm (denom) of weight_pos
        updated_weight = weight_pos.div(w_norm) # l2 norm the addition of backbone result and the existing weights from bank (with momentum proportion)  
        memory.index_copy_(0, y, updated_weight)
            # update the values of memory bank, at indexes of the batch data (y) that has been put through
        
        return gradInput, None, None, None, None
```


```python
class NCEAverage(nn.Module):
    # handles part 3.2. in paper
    def __init__(self, inputSize, outputSize, K, T=0.07, momentum=0.5, Z=None):
        super(NCEAverage, self).__init__()
        self.nLem = outputSize # number of data in the training dataset
        
        self.unigrams = torch.ones(self.nLem) # nLem-dimensional matrix of value 1s
        self.multinomial = AliasMethod(self.unigrams) # see AliasMethod function below
        self.multinomial.cuda()
        
        self.K = K # nce_k or 'm' specified by paper

        self.register_buffer('params',torch.tensor([K, T, -1, momentum])) # store nce_k, tau, momentum in parameter buffer
        
        stdv = 1. / math.sqrt(inputSize/3)
        self.register_buffer('memory', torch.rand(outputSize, inputSize).mul_(2*stdv).add_(-stdv))
            # initialize memory bank, uniform distributed and dependent on stdv (which is in turn dependent on the dimension we want to save "mini" image on the bank)
 
    def forward(self, x, y):
        # x - features; output of the data passed from the backbone (latent_dim)
        # y - the indexes of the data being put in (corresponding indexes to the features)
        batchSize = x.size(0)
        
        idx = self.multinomial.draw(batchSize * (self.K+1)).view(batchSize, -1)
            # get list of indexes from AliasMethod.draw(), 2 dimensional (batchSize x K+1) - also notated as multinomial
        out = NCEFunction.apply(x, y, self.memory, idx, self.params)
            # get P(i|v), from paper 3.2(4) based on features, indexes (of a batch), memory bank, indexes from above, and fixed parameters (saved)
        
        return out
```


```python
nce_k = 4096
nce_t = 0.07
nce_m = 0.5
```


```python
if nce_k > 0:
    lemniscate = NCEAverage(low_dim, ndata, nce_k, nce_t, nce_m)
else:
    print('nce_k value must be above zero.')
```

<h4>Non-parametric softmax classifier, with noise-contrastive estimation - define objective function</h4>

Given P(i|v) (\*3.4) from NCEAverage, and indexes (targets) of a batch of data, calculate the objective function (\*3.10)


```python
eps = 1e-7

class NCECriterion(nn.Module):

    def __init__(self, nLem):
        super(NCECriterion, self).__init__()
        self.nLem = nLem

    def forward(self, x, targets):
        # x - 2 dimensional, batchsize x (K+1), representing (*3.4)
        # targets - (batchsize number of) indexes
        
        batchSize = x.size(0)
        K = x.size(1)-1 # K
        Pnt = 1 / float(self.nLem)
        Pns = 1 / float(self.nLem)
        
        # P(origin=model) = Pmt / (Pmt + k*Pnt) {eq. (*3.6)} 
        Pmt = x.select(1,0) # real (nominator - *3.4)
        Pmt_div = Pmt.add(K * Pnt + eps) # the denominator of h(i,v) in paper 3.2(6) (*3.5)
        lnPmt = torch.div(Pmt, Pmt_div) # h(i,v) (*3.6)
        
        # P(origin=noise) = k*Pns / (Pms + k*Pns) {eq. (*3.9)}
        Pon_div = x.narrow(1,1,K).add(K * Pns + eps) # denominator of h(i,v') in paper 3.2(6), (*3.7)
        Pon = Pon_div.clone().fill_(K * Pns) # prepare for 1-h(i,v'), (*3.8)
        lnPon = torch.div(Pon, Pon_div) # 1-h(i,v') for 3.2(7), (*3.9)
     
        # 3.2(7) from paper, log and apply expected value over given distribution (d and n)
        lnPmt.log_()
        lnPon.log_()        
        lnPmtsum = lnPmt.sum(0)
        lnPonsum = lnPon.view(-1, 1).sum(0)        
        loss = - (lnPmtsum + lnPonsum) / batchSize # objective function form complete (*3.10)
        
        return loss
```


```python
if hasattr(lemniscate, 'K'):
    criterion = NCECriterion(ndata)
else:
    print('nce_k = 0')
```

<h4>4. Begin training the model</h4>
<h5>Training Procedure</h5>
<img src="/assets/images/nonparam_files/np_training_procedure.png">
<br />


```python
net.to(device)
lemniscate.to(device)
criterion.to(device)
```


```python
# define optimizer, with custom adjustment of learning rate

import torch.optim as optim

starting_lr = 0.03

optimizer = optim.SGD(net.parameters(), lr=starting_lr, momentum=0.9, weight_decay=5e-4)

def adjust_learning_rate(optimizer, epoch):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    lr = starting_lr
    if epoch >= 80:
        lr = starting_lr * (0.1 ** ((epoch-80) // 40))
    print('Learning rate of this epoch: {}'.format(lr))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
```


```python
# utility class that updates value and average value for display when training the model 
# displaying (print) for our references only, will not affect performance of model

class AverageMeter(object):
    """Computes and stores the average and current value""" 
    def __init__(self):
        self.reset()
                   
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0 

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
```


```python
def train(epoch):
    print('\nEpoch: %d' % epoch)
    adjust_learning_rate(optimizer, epoch)
    train_loss = AverageMeter()
    correct = 0
    total = 0

    # switch to train mode
    net.train()

    end = time.time()
    for batch_idx, (inputs, targets, indexes) in enumerate(trainloader):
        inputs, targets, indexes = inputs.to(device), targets.to(device), indexes.to(device)
        # inputs - pictures - batch x data_dim - 4 dimensional: 128x3x32x32
        # targets - class label 0~9 - 1 dim: 128
        # indexes - index of data among the dataset - 1 dim: 128
        optimizer.zero_grad() # reset optimizer gradients

        features = net(inputs)
        # go through backbone, output features 128x128 : batch x low_dim
        outputs = lemniscate(features, indexes)
        # adjust memory bank, calculate P(i|v), outputs 128x4097 : batch x nce_k
        
        loss = criterion(outputs, indexes)
        # objective function using NCE method - loss scalar
        loss.backward()
        optimizer.step() # perform back propagation

        # for display
        train_loss.update(loss.item(), inputs.size(0))
        
        if batch_idx >= len(trainloader)-1:
            time_taken = time.time() - end
            print('Epoch {} complete. [batchs trained: {}]\n'
                  'Time taken: {:.3f}, '
                  'Avg Loss: {train_loss.avg:.4f}'.format(
                  epoch, len(trainloader), time_taken, train_loss=train_loss))
```


```python
total_epochs = 200

for epoch in range(total_epochs):
    train(epoch)
```

<h5>Learning curve</h5>
<img src="/assets/images/nonparam_files/np_learningcurve.png" >
<br />

```python
# save model weights / lemniscate buffers (memory bank specifically)
# for future further training, or evaluation

state = {
            'net': net.state_dict(),
            'lemniscate': lemniscate,
        }

if not os.path.isdir('checkpoint_'):
    os.mkdir('checkpoint_')

torch.save(state, './checkpoint_/cp.t7')
```

<h4>5. Evaluation - extract candidates</h4>
<h5>Evaluation procedure</h5>
<br />
<img src="/assets/images/nonparam_files/np_evaluation_procedure.png">
<br />


```python
# optional code: if you have checkpoint saved
# this block is to load the learned backbone/memory bank weights

import torch
import torch.backends.cudnn as cudnn

checkpoint = torch.load('./checkpoint_/cp.t7')

low_dim = 128
net = ResNet18(low_dim)
net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))
cudnn.benchmark = True

lemniscate = NCEAverage(128, 50000, 4096, 0.07, 0.5)

net.load_state_dict(checkpoint['net'])
lemniscate = checkpoint['lemniscate']

device = 'cuda'
net.to(device)
lemniscate.to(device)
```



```python
def kNN_unsupervised(net, lemniscate, testloader):
    
    net.eval()
    total = 0
    testsize = testloader.dataset.__len__() # 10000 for cifar

    trainFeatures = lemniscate.memory.t() # transposed features from memory bank (representing every image in the bank - cifar 50k)
    
    sorted_candidates = None
    
    timestart = time.time()
    
    with torch.no_grad():
        
        for batch_idx, (inputs, targets, indexes) in enumerate(testloader):
            targets = targets.cuda(async=True)
            batchSize = inputs.size(0)
            features = net(inputs)
            end = time.time()

            dist = torch.mm(features, trainFeatures) # "cosine-similarity"
            
            yd, yi = torch.sort(dist, dim=1, descending=True) # sort for each row, from maximum cosine similarity to minimum
            
            if sorted_candidates is None:
                sorted_candidates = yi.detach().to('cpu').numpy()
            else:
                sorted_candidates = np.concatenate((sorted_candidates, yi.detach().to('cpu').numpy()), axis=0)
                        
            total += targets.size(0)
    
    timetaken = time.time() - timestart
    print('Time taken: {:.2f} seconds'.format(timetaken))
        
    return sorted_candidates
```


```python
test_candidates = kNN_unsupervised(net, lemniscate, testloader)
```

    Time taken: 50.48 seconds



```python
# function input : numpy array of pixel values (picture)
# output : picture saved to disk in png, also display
def imageplot(numpy_arr, name='output.png'):
    x_size = np.max([10, numpy_arr.shape[1] / 100])
    y_size = np.max([7, numpy_arr.shape[0] / 100])
    plt.figure(figsize=(x_size,y_size))
    plt.imshow(numpy_arr)
    plt.axis('off')
    plt.savefig(name)
    plt.show()

# test_data : test images in RGB, numpy array of shape (number of images)x(y dimension)x(x dimension)x3(RGB)
# bank : images in a bank to reference from candidates (train), same structure as test_data
# candidates : indexes to pick out certain images from bank
# test_image_indexes : indexes to pick out certain images from test_data
# num_candidates : number of images to choose from candidates
def display_candidates(test_data, bank, candidates, test_image_indexes=[1,2,3,4,5], num_candidates=10, desc=False):
    
    total_image_np = None

    for i in test_image_indexes:
        total_image_row = None
        for j in range(num_candidates):
            entry_num = j
            if desc:
                entry_num = candidates.shape[1] - j - 1
            if total_image_row is None:
                total_image_row = np.hstack((test_data[i], bank[candidates[i,entry_num]]))
            else:
                total_image_row = np.hstack((total_image_row, bank[candidates[i,entry_num]]))
        if total_image_np is None:
            total_image_np = total_image_row
        else:
            total_image_np = np.vstack((total_image_np, total_image_row))

    imageplot(total_image_np, name='output.png')
```


```python
display_candidates(testloader.dataset.test_data, trainloader.dataset.train_data, test_candidates,
                  test_image_indexes=list(range(1000,1010)),
                  num_candidates=10)
```


![png](/assets/images/nonparam_files/output_37_0.png)


<h4>Further application - classification problem (measure accuracy)</h4>


```python
def kNN(net, lemniscate, trainloader, testloader, K, sigma, recompute_memory=0):
    net.eval()
    
    total = 0
    testsize = testloader.dataset.__len__() # 10000 for cifar

    trainFeatures = lemniscate.memory.t() # t() means transpose, features from memory bank (representing every image in the bank - cifar 50k)
    if hasattr(trainloader.dataset, 'imgs'):
        trainLabels = torch.LongTensor([y for (p, y) in trainloader.dataset.imgs]).cuda()
    else: # goes to here for CIFAR10
        trainLabels = torch.LongTensor(trainloader.dataset.train_labels).cuda()
    C = trainLabels.max() + 1 # 9 + 1 = 10

    if recompute_memory:
        transform_bak = trainloader.dataset.transform
        trainloader.dataset.transform = testloader.dataset.transform
        temploader = torch.utils.data.DataLoader(trainloader.dataset, batch_size=100, shuffle=False, num_workers=1)
        for batch_idx, (inputs, targets, indexes) in enumerate(temploader):
            targets = targets.cuda(async=True)
            batchSize = inputs.size(0)
            features = net(inputs)
            trainFeatures[:, batch_idx*batchSize:batch_idx*batchSize+batchSize] = features.data.t()
        trainLabels = torch.LongTensor(temploader.dataset.train_labels).cuda()
        trainloader.dataset.transform = transform_bak
    
    top1 = 0.
    top5 = 0.
    end = time.time()
    with torch.no_grad():
        retrieval_one_hot = torch.zeros(K, C).cuda()
        for batch_idx, (inputs, targets, indexes) in enumerate(testloader):
            targets = targets.cuda(async=True)
            batchSize = inputs.size(0)
            features = net(inputs)

            dist = torch.mm(features, trainFeatures) # "cosine-similarity" s(i) in *5.1
            yd, yi = dist.topk(K, dim=1, largest=True, sorted=True) # find K(200) largest values for each row in dist, storing dist values and indexes of where that dist is
            candidates = trainLabels.view(1,-1).expand(batchSize, -1) # copy training data labels (50k) to batchSize (100) amount of rows (100x50000)
            retrieval = torch.gather(candidates, 1, yi) # candidates classes (k=200 so -retrieval: 100x200 - batchSize x k 2dimensional)

            retrieval_one_hot.resize_(batchSize * K, C).zero_()
            retrieval_one_hot.scatter_(1, retrieval.view(-1, 1), 1) # candidates classes one hotted
            yd_transform = yd.clone().div_(sigma).exp_() # *5.1
            
            probs = torch.sum(torch.mul(retrieval_one_hot.view(batchSize, -1 , C), yd_transform.view(batchSize, -1, 1)), 1) # *5.2, completes formula in 3.4 in paper
            _, predictions = probs.sort(1, True)

            # Find which predictions match the target (top1, top5 acc calculations only)
            correct = predictions.eq(targets.data.view(-1,1))

            top1 = top1 + correct.narrow(1,0,1).sum().item()
            top5 = top5 + correct.narrow(1,0,5).sum().item()

            total += targets.size(0)
    
    timetaken = time.time() - end
    print('Time taken: {:.4f} seconds'.format(timetaken))
    print('Top 1 acc: {:.2f} %'.format(top1*100./total))
    print('Top 5 acc: {:.2f} %'.format(top5*100./total))
    
    return top1/total
```


```python
acc = kNN(net, lemniscate, trainloader, testloader, 200, nce_t, 0) # measure topn accuracy based on model from last epoch
```

    Time taken: 1.9918 seconds
    Top 1 acc: 72.10 %
    Top 5 acc: 95.98 %


<h3>Formula used in code</h3>

The following formulae are processed, and referred to, in code used in this page:

<br />

\*3.1. $v^{\top}f_{i}$ , 

where $f_{i}$ is feature computed by backbone of $i$<sup>th</sup> sample, $v$ memory bank at $i$
<br /><br />

\*3.2. $\exp({v^{\top}f_{i}}/{\tau})$ , 

where $\tau$ is constant, $v^{\top}f_{i}$ is from \*3.1
<br /><br />

\*3.3. $Z_{i} {\approx} {\frac{n}{m}}\sum_{k=1}^{m}{\exp({v^{\top}f_{j_{k}}}/{\tau})}$

By definition, $Z_{i} = \sum_{i=1}^{n}{\exp({v^{\top}f_{i}}/{\tau})}$ <b>(#)</b>, using \*3.2 summing across $n$.

The above calculation <b>(#)</b> is known to be computationally expensive to evaluate (with large $n$),

thus in our method (code), we treat it as a constant and estimate its value via Monte Carlo approximation.
<br /><br />

\*3.4. $P(i | v) = \frac{\exp({v^{\top}f_{i}}/{\tau})}{Z_{i}}$

uses \*3.2 as nominator and \*3.3 as denominator
<br /><br />

\*3.5. $P(i | v) + m{P_{n}(i)}$

$P(i|v)$ from \*3.4 represents probability in data samples,

${P_{n}(i)}$ represents probability of class $i$ under noise distribution - 

uniform over $n$ number of training data, $P_{n} = 1/{n}$, $m$ constant (number of noise samples)
<br /><br />

\*3.6. $h(i, v) := P(D=1|i,v)$ = $\frac{P(i | v)}{P(i | v) + m{P_{n}(i)}}$

uses \*3.4 as nominator and \*3.5 as denominator
<br /><br />

\*3.7. $P(i | v') + m{P_{n}(i)}$

where $v'$ is the feature from another image, randomly sampled according to noise distribution $P_{n}$
<br /><br />

\*3.8. $m{P_{n}(i)}$

described in \*3.5
<br /><br />

\*3.9.  $1 - h(i,v') = 1 - \frac{P(i|v')}{P(i|v') + m{P_{n}(i)}} = \frac{m{P_{n}(i)}}{P(i|v') + m{P_{n}(i)}}$

uses \*3.8 as nominator and \*3.7 as denominator
<br /><br />

\*3.10. $J_{NCE}(\theta) = -E_{P_{d}}[\log h(i,v)] - m{\cdot}E_{P_{n}}[\log (1-h(i,v'))]$

uses \*3.6, \*3.9 to complete the definition of the objective function used here.
<br /><br />

\*5.1. ${\alpha}_{i} = \exp({s_{i}}/{\tau})$ ,

where $s_{i} = \cos(v_{i},\hat{f})$, cosine-similarity between test data's computed features and i<sup>th</sup> entry in memory bank.
<br /><br />

\*5.2. $w_{c} = \sum_{i\in N_{k}}{\alpha_{i}\cdot 1(c_{i}=c)}$

where, for $i$ in $N_{k}$ top k nearest neighbours, 

$\alpha_{i}$ is summed for all memory bank entries $i$ that match the label of test data label.

We call $w_{c}$ "total weight of class $c$".
<br /><br />

<br />
References:

(1)
<cite>
wu2018unsupervised,
  title={Unsupervised Feature Learning via Non-Parametric Instance Discrimination},
  author={Wu, Zhirong and Xiong, Yuanjun and Stella, X Yu and Lin, Dahua},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2018}
</cite>

Disclaimer:

This work is done purely for educational purposes thus non-commercial. 

